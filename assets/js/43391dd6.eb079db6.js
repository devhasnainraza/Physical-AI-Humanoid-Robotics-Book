"use strict";(globalThis.webpackChunkcortex_h1=globalThis.webpackChunkcortex_h1||[]).push([[1581],{5209:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"labs/lab-4-visual-slam","title":"Lab 4: Visual SLAM for Environment Mapping","description":"Objective","source":"@site/docs/labs/lab-4-visual-slam.md","sourceDirName":"labs","slug":"/labs/lab-4-visual-slam","permalink":"/Cortex-H1/docs/labs/lab-4-visual-slam","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/labs/lab-4-visual-slam.md","tags":[],"version":"current","frontMatter":{"id":"lab-4-visual-slam","title":"Lab 4: Visual SLAM for Environment Mapping","sidebar_label":"Lab 4: Visual SLAM"},"sidebar":"textbookSidebar","previous":{"title":"Lab 3: PID Controller","permalink":"/Cortex-H1/docs/labs/lab-3-pid-controller"},"next":{"title":"Lab 5: RL Locomotion","permalink":"/Cortex-H1/docs/labs/lab-5-rl-locomotion"}}');var s=a(4848),r=a(8453);const o={id:"lab-4-visual-slam",title:"Lab 4: Visual SLAM for Environment Mapping",sidebar_label:"Lab 4: Visual SLAM"},t="Lab 4: Visual SLAM for Environment Mapping",l={},c=[{value:"Objective",id:"objective",level:2},{value:"Theoretical Background",id:"theoretical-background",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step-by-Step Instructions",id:"step-by-step-instructions",level:2},{value:"Step 1: Prepare Your Workspace for Isaac ROS",id:"step-1-prepare-your-workspace-for-isaac-ros",level:3},{value:"Step 2: Create a Gazebo World with a Stereo Camera Robot",id:"step-2-create-a-gazebo-world-with-a-stereo-camera-robot",level:3},{value:"Step 3: Build and Launch Simulation",id:"step-3-build-and-launch-simulation",level:3},{value:"Step 4: Launch Isaac ROS Visual SLAM",id:"step-4-launch-isaac-ros-visual-slam",level:3},{value:"Step 5: Visualize SLAM Output in RViz",id:"step-5-visualize-slam-output-in-rviz",level:3},{value:"Step 6: Drive the Robot and Build Map",id:"step-6-drive-the-robot-and-build-map",level:3},{value:"Verification",id:"verification",level:2},{value:"Challenge Questions",id:"challenge-questions",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lab-4-visual-slam-for-environment-mapping",children:"Lab 4: Visual SLAM for Environment Mapping"})}),"\n",(0,s.jsx)(n.h2,{id:"objective",children:"Objective"}),"\n",(0,s.jsxs)(n.p,{children:["This lab focuses on ",(0,s.jsx)(n.strong,{children:"Visual SLAM (Simultaneous Localization and Mapping)"}),". You will launch a simulated robot with a stereo camera in Gazebo, use the NVIDIA Isaac ROS Visual SLAM package to estimate the robot's pose and build a sparse map of the environment, and visualize the results in RViz."]}),"\n",(0,s.jsx)(n.h2,{id:"theoretical-background",children:"Theoretical Background"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Visual SLAM"})," solves the problem of simultaneously building a map of an unknown environment and localizing the robot within that map using only camera imagery. For a detailed theoretical understanding, refer to ",(0,s.jsx)(n.strong,{children:"Chapter 8: SLAM and Navigation"}),"."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stereo Camera"}),": Provides two image streams (left and right) that allow for triangulation and direct depth estimation, improving robustness over monocular VSLAM."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Matching"}),": Identifying and tracking unique points across multiple camera frames to estimate ego-motion."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pose Graph Optimization"}),": A backend optimization process that reduces accumulated errors, especially during loop closures."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Humble Development Environment"}),": Set up as in Lab 1."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"NVIDIA Isaac ROS"}),": Requires a Jetson Orin device or an NVIDIA GPU on your workstation with a compatible Isaac ROS Docker container setup. For this lab, we will use a pre-configured Docker image (or assume a workstation with GPU and Isaac ROS installed)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gazebo Harmonic (Ignition)"}),": Installed with ROS 2.","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo apt install ros-humble-ros-gz\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"step-by-step-instructions",children:"Step-by-Step Instructions"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-prepare-your-workspace-for-isaac-ros",children:"Step 1: Prepare Your Workspace for Isaac ROS"}),"\n",(0,s.jsx)(n.p,{children:"Due to the dependencies and GPU requirements of Isaac ROS, it's often easiest to work within a dedicated Docker container."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pull Isaac ROS Container"}),":","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker pull nvcr.io/nvidia/isaac-ros-dev:humble\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Launch Container (if on native Ubuntu/Jetson)"}),":\nIf you are on an NVIDIA Jetson or a native Ubuntu machine with an NVIDIA GPU, you can launch the container directly. Skip the DevContainer and launch it from your host machine.","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker run -it --rm --network host --runtime nvidia \\\n    -e ROS_DOMAIN_ID=$ROS_DOMAIN_ID \\\n    -v /path/to/this/book/project:/workspaces/cortex-h1 \\\n    nvcr.io/nvidia/isaac-ros-dev:humble bash\n# Replace /path/to/this/book/project with your actual path\n"})}),"\n",(0,s.jsxs)(n.em,{children:["Note: If you are already in a Dev Container based on ",(0,s.jsx)(n.code,{children:"ros-humble"}),", you might need to ensure it has GPU access and the necessary Isaac ROS packages. For simplicity, we will assume you are either in a suitable Isaac ROS container or a native setup."]})]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-2-create-a-gazebo-world-with-a-stereo-camera-robot",children:"Step 2: Create a Gazebo World with a Stereo Camera Robot"}),"\n",(0,s.jsx)(n.p,{children:"We will launch a simple robot with a stereo camera."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Create a new ROS 2 package:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_cmake my_slam_sim\ncd my_slam_sim\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Create a URDF for a simple robot with a stereo camera (",(0,s.jsx)(n.code,{children:"urdf/stereo_robot.urdf"}),"):","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="stereo_robot">\n  <link name="base_link">\n    <visual><geometry><box size="0.2 0.2 0.1"/></geometry></visual>\n    <collision><geometry><box size="0.2 0.2 0.1"/></geometry></collision>\n    <inertial><mass value="1.0"/><inertia ixx="0.01" iyy="0.01" izz="0.01"/></inertial>\n  </link>\n\n  <joint name="camera_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="camera_link"/>\n    <origin xyz="0.1 0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  <link name="camera_link">\n    <visual><geometry><box size="0.05 0.1 0.05"/></geometry></visual>\n    <collision><geometry><box size="0.05 0.1 0.05"/></geometry></collision>\n    <inertial><mass value="0.01"/><inertia ixx="0.0001" iyy="0.0001" izz="0.0001"/></inertial>\n    <sensor name="stereo_camera" type="multicamera">\n      <always_on>true</always_on>\n      <update_rate>10.0</update_rate>\n      <camera name="left_camera">\n        <horizontal_fov>1.047</horizontal_fov>\n        <image><width>640</width><height>480</height><format>R8G8B8</format></image>\n        <clip><near>0.1</near><far>100</far></clip>\n        <pose>0 -0.05 0 0 0 0</pose> \x3c!-- Left camera slightly to the left --\x3e\n      </camera>\n      <camera name="right_camera">\n        <horizontal_fov>1.047</horizontal_fov>\n        <image><width>640</width><height>480</height><format>R8G8B8</format></image>\n        <clip><near>0.1</near><far>100</far></clip>\n        <pose>0 0.05 0 0 0 0</pose> \x3c!-- Right camera slightly to the right --\x3e\n      </camera>\n      <plugin name="camera_controller" filename="libgazebo_ros_multicamera.so">\n        <cameraName>stereo_camera</cameraName>\n        <alwaysOn>true</alwaysOn>\n        <updateRate>10.0</updateRate>\n        <imageTopicName>image_raw</imageTopicName>\n        <cameraInfoTopicName>camera_info</cameraInfoTopicName>\n        <frameName>camera_link</frameName>\n        <hack_baseline>0.1</hack_baseline>\n        <tf_prefix>stereo_camera</tf_prefix>\n      </plugin>\n    </sensor>\n  </link>\n</robot>\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Create a launch file to spawn the robot in Gazebo (",(0,s.jsx)(n.code,{children:"launch/stereo_sim.launch.py"}),"):","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import os\nfrom ament_index_python.packages import get_package_share_directory\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription, DeclareLaunchArgument\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch_ros.actions import Node\nfrom launch.substitutions import LaunchConfiguration\n\ndef generate_launch_description():\n    pkg_name = 'my_slam_sim'\n    pkg_share_dir = get_package_share_directory(pkg_name)\n    \n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n\n    robot_description_path = os.path.join(pkg_share_dir, 'urdf', 'stereo_robot.urdf')\n    \n    # Gazebo launch\n    gazebo_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource(\n            os.path.join(get_package_share_directory('ros_gz_sim'), 'launch', 'gz_sim.launch.py')\n        ),\n        launch_arguments={'gz_args': '-r empty.sdf'}.items() # -r starts paused\n    )\n\n    # Robot State Publisher (publishes TF from URDF)\n    robot_state_publisher_node = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        name='robot_state_publisher',\n        output='screen',\n        parameters=[{'use_sim_time': use_sim_time, 'robot_description': open(robot_description_path).read()}],\n        arguments=[robot_description_path] # Pass URDF directly\n    )\n\n    # Spawn robot in Gazebo\n    spawn_entity_node = Node(\n        package='ros_gz_sim',\n        executable='create',\n        arguments=['-name', 'stereo_robot', '-topic', 'robot_description', '-x', '0', '-y', '0', '-z', '0.5'],\n        output='screen'\n    )\n\n    # Bridge Gazebo images to ROS 2 topics\n    # Note: Gazebo plugin should already publish to ROS 2. This is a fallback/additional bridge.\n    bridge_node = Node(\n        package='ros_gz_bridge',\n        executable='parameter_bridge',\n        arguments=[\n            '/stereo_camera/left/image_raw@sensor_msgs/msg/Image[gz.msgs.Image',\n            '/stereo_camera/right/image_raw@sensor_msgs/msg/Image[gz.msgs.Image',\n            '/stereo_camera/left/camera_info@sensor_msgs/msg/CameraInfo[gz.msgs.CameraInfo',\n            '/stereo_camera/right/camera_info@sensor_msgs/msg/CameraInfo[gz.msgs.CameraInfo',\n        ],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        DeclareLaunchArgument('use_sim_time', default_value='true', description='Use simulation (Gazebo) clock if true'),\n        gazebo_launch,\n        robot_state_publisher_node,\n        spawn_entity_node,\n        bridge_node\n    ])\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Update ",(0,s.jsx)(n.code,{children:"my_slam_sim/CMakeLists.txt"})," and ",(0,s.jsx)(n.code,{children:"my_slam_sim/package.xml"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CMakeLists.txt"}),": Add ",(0,s.jsx)(n.code,{children:"install(DIRECTORY urdf launch DESTINATION share/${PROJECT_NAME})"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"package.xml"}),": Add ",(0,s.jsx)(n.code,{children:"ros_gz_sim"}),", ",(0,s.jsx)(n.code,{children:"robot_state_publisher"}),", ",(0,s.jsx)(n.code,{children:"ros_gz_bridge"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-3-build-and-launch-simulation",children:"Step 3: Build and Launch Simulation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws\ncolcon build --packages-select my_slam_sim\n. install/setup.bash\nros2 launch my_slam_sim stereo_sim.launch.py\n"})}),"\n",(0,s.jsx)(n.p,{children:"This should open Gazebo with your robot."}),"\n",(0,s.jsx)(n.h3,{id:"step-4-launch-isaac-ros-visual-slam",children:"Step 4: Launch Isaac ROS Visual SLAM"}),"\n",(0,s.jsx)(n.p,{children:"In a new terminal (within your Isaac ROS environment/container):"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:". install/setup.bash # Source your workspace\nros2 launch isaac_ros_visual_slam visual_slam.launch.py \\\n    # Map topics from our simulation to VSLAM\n    left_image_topic:=/stereo_camera/left/image_raw \\\n    left_camera_info_topic:=/stereo_camera/left/camera_info \\\n    right_image_topic:=/stereo_camera/right/image_raw \\\n    right_camera_info_topic:=/stereo_camera/right/camera_info \\\n    # Set to true for a simple empty world mapping\n    enable_imu_fusion:=false \\\n    # Isaac ROS will publish its output here\n    publish_tf:=true \\\n    use_sim_time:=true\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-5-visualize-slam-output-in-rviz",children:"Step 5: Visualize SLAM Output in RViz"}),"\n",(0,s.jsx)(n.p,{children:"Open another terminal, source your workspace, and launch RViz:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:". install/setup.bash\nrviz2 -d $(ros2 pkg prefix my_slam_sim)/share/my_slam_sim/rviz/slam.rviz # You'll create this file\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Create ",(0,s.jsx)(n.code,{children:"rviz/slam.rviz"})]}),": Save the current RViz configuration (",(0,s.jsx)(n.code,{children:"File -> Save Config As"}),") after setting up the following displays:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"TF"}),": To see the robot's pose."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Image"}),": For ",(0,s.jsx)(n.code,{children:"/stereo_camera/left/image_raw"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"PointCloud2"}),": For ",(0,s.jsx)(n.code,{children:"/visual_slam/map"})," (or similar map topic from Isaac ROS)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Path"}),": For ",(0,s.jsx)(n.code,{children:"/visual_slam/tracking/odometry"})," to see the estimated path."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-6-drive-the-robot-and-build-map",children:"Step 6: Drive the Robot and Build Map"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["In Gazebo, make sure the simulation is running (",(0,s.jsx)(n.code,{children:"play"})," button if paused)."]}),"\n",(0,s.jsx)(n.li,{children:"Use the Gazebo GUI controls to drive your stereo robot around the empty world."}),"\n",(0,s.jsxs)(n.li,{children:["Observe the RViz window. You should see:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The camera image stream."}),"\n",(0,s.jsx)(n.li,{children:"The robot's pose moving according to your control."}),"\n",(0,s.jsx)(n.li,{children:"A sparse point cloud map building up in real-time."}),"\n",(0,s.jsx)(n.li,{children:"The estimated path of the robot."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"verification",children:"Verification"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Gazebo window with stereo robot appears."}),"\n",(0,s.jsx)(n.li,{children:"Isaac ROS Visual SLAM launches without errors."}),"\n",(0,s.jsx)(n.li,{children:"RViz displays robot pose, camera images, and a point cloud map being built as the robot moves."}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"/visual_slam/tracking/odometry"})," topic should show changes in pose."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"challenge-questions",children:"Challenge Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Map Density"}),": Add some simple textured boxes or cylinders to your Gazebo world (",(0,s.jsx)(n.code,{children:".sdf"})," file). How does this affect the density and accuracy of the generated point cloud map?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMU Fusion"}),": Modify the ",(0,s.jsx)(n.code,{children:"visual_slam.launch.py"})," to enable IMU fusion (",(0,s.jsx)(n.code,{children:"enable_imu_fusion:=true"}),") and ensure your robot's URDF includes an IMU sensor with a corresponding topic. How does IMU data help stabilize the map and pose estimation, especially during fast movements?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Loop Closure"}),': Drive the robot in a loop (e.g., a square path) and return to its starting point. Does the map "snap" together? If not, what parameters in VSLAM might you tune to improve loop closure detection?']}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>t});var i=a(6540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);