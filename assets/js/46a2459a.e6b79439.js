"use strict";(globalThis.webpackChunkcortex_h1=globalThis.webpackChunkcortex_h1||[]).push([[5975],{7401:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"module-3-ai-brain/q-learning","title":"3.3 Lab: Training with DQN","description":"Using Stable Baselines 3 to solve CartPole.","source":"@site/docs/module-3-ai-brain/03-q-learning.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/q-learning","permalink":"/Cortex-H1/docs/module-3-ai-brain/q-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-ai-brain/03-q-learning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"q-learning","title":"3.3 Lab: Training with DQN","sidebar_label":"3.3 Lab: DQN","description":"Using Stable Baselines 3 to solve CartPole."},"sidebar":"textbookSidebar","previous":{"title":"3.2 Lab: Custom Env","permalink":"/Cortex-H1/docs/module-3-ai-brain/mdp"},"next":{"title":"3.4 Lab: PPO","permalink":"/Cortex-H1/docs/module-3-ai-brain/policy-gradients"}}');var t=i(4848),a=i(8453);const s={id:"q-learning",title:"3.3 Lab: Training with DQN",sidebar_label:"3.3 Lab: DQN",description:"Using Stable Baselines 3 to solve CartPole."},o="3.3 Lab: Training with DQN",l={},d=[{value:"\ud83c\udfaf Lab Objectives",id:"-lab-objectives",level:2},{value:"3.3.1 The Training Script",id:"331-the-training-script",level:2},{value:"3.3.2 Monitoring with Tensorboard",id:"332-monitoring-with-tensorboard",level:2},{value:"3.3.3 Evaluation (Enjoy Mode)",id:"333-evaluation-enjoy-mode",level:2},{value:"3.3.4 Quiz",id:"334-quiz",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"33-lab-training-with-dqn",children:"3.3 Lab: Training with DQN"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:'"From Randomness to Mastery."'})}),"\n",(0,t.jsxs)(n.p,{children:["We have the environment. Now we attach the Brain. We will use ",(0,t.jsx)(n.strong,{children:"Deep Q-Network (DQN)"}),", the algorithm that started the Deep RL revolution (Atari)."]}),"\n",(0,t.jsx)(n.h2,{id:"-lab-objectives",children:"\ud83c\udfaf Lab Objectives"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Train a DQN Agent"})," using SB3."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Monitor Training"})," with Tensorboard."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Save and Load"})," the trained model."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"331-the-training-script",children:"3.3.1 The Training Script"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import gymnasium as gym\nfrom stable_baselines3 import DQN\nimport os\n\n# 1. Create Environment\nenv = gym.make("CartPole-v1", render_mode="rgb_array")\n\n# 2. Define Model\n# MlpPolicy: Multi-Layer Perceptron (Simple Neural Net)\nmodel = DQN("MlpPolicy", env, verbose=1, tensorboard_log="./dqn_cartpole_logs/")\n\n# 3. Train\nprint("Training started...")\nmodel.learn(total_timesteps=50000)\nprint("Training finished.")\n\n# 4. Save\nmodel.save("dqn_cartpole")\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"332-monitoring-with-tensorboard",children:"3.3.2 Monitoring with Tensorboard"}),"\n",(0,t.jsx)(n.p,{children:"While training runs, open a new terminal:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"tensorboard --logdir ./dqn_cartpole_logs/\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Go to ",(0,t.jsx)(n.code,{children:"http://localhost:6006"}),"."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsxs)(n.strong,{children:["Watch the ",(0,t.jsx)(n.code,{children:"rollout/ep_rew_mean"})," graph."]})}),"\n",(0,t.jsx)(n.li,{children:"It should start at ~20 and rise to 500 (Max score) within 50k steps."}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"333-evaluation-enjoy-mode",children:"3.3.3 Evaluation (Enjoy Mode)"}),"\n",(0,t.jsx)(n.p,{children:"Now let's watch the trained agent."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Load Model\nmodel = DQN.load("dqn_cartpole")\n\n# Create Env with Render\nenv = gym.make("CartPole-v1", render_mode="human")\n\nobs, _ = env.reset()\nfor _ in range(1000):\n    env.render()\n    \n    # Predict Action (Deterministic)\n    action, _states = model.predict(obs, deterministic=True)\n    \n    obs, reward, done, truncated, info = env.step(action)\n    if done or truncated:\n        obs, _ = env.reset()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"334-quiz",children:"3.3.4 Quiz"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:["Why do we set ",(0,t.jsx)(n.code,{children:"deterministic=True"})," during evaluation?"]})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"a) To stop the agent from exploring (Epsilon-greedy) and make it perform its best."}),"\n",(0,t.jsx)(n.li,{children:"b) To save memory."}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.em,{children:"Answer: a"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"If the reward graph stays flat at 20, what happened?"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"a) The agent failed to learn (Hyperparameters might be wrong)."}),"\n",(0,t.jsx)(n.li,{children:"b) The task is impossible."}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.em,{children:"Answer: a"})}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var r=i(6540);const t={},a=r.createContext(t);function s(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);