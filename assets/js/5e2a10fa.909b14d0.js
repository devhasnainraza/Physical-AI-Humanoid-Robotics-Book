"use strict";(globalThis.webpackChunkcortex_h1=globalThis.webpackChunkcortex_h1||[]).push([[4262],{1093:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"labs/lab-5-rl-locomotion","title":"Lab 5: Deep Reinforcement Learning for Locomotion","description":"Objective","source":"@site/docs/labs/lab-5-rl-locomotion.md","sourceDirName":"labs","slug":"/labs/lab-5-rl-locomotion","permalink":"/Cortex-H1/docs/labs/lab-5-rl-locomotion","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/labs/lab-5-rl-locomotion.md","tags":[],"version":"current","frontMatter":{"id":"lab-5-rl-locomotion","title":"Lab 5: Deep Reinforcement Learning for Locomotion","sidebar_label":"Lab 5: RL Locomotion"},"sidebar":"textbookSidebar","previous":{"title":"Lab 4: Visual SLAM","permalink":"/Cortex-H1/docs/labs/lab-4-visual-slam"},"next":{"title":"Lab 6: LLM Planner","permalink":"/Cortex-H1/docs/labs/lab-6-llm-planner"}}');var i=t(4848),r=t(8453);const a={id:"lab-5-rl-locomotion",title:"Lab 5: Deep Reinforcement Learning for Locomotion",sidebar_label:"Lab 5: RL Locomotion"},s="Lab 5: Deep Reinforcement Learning for Locomotion",l={},c=[{value:"Objective",id:"objective",level:2},{value:"Theoretical Background",id:"theoretical-background",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step-by-Step Instructions",id:"step-by-step-instructions",level:2},{value:"Step 1: Create a Custom Legged Robot Environment in Gymnasium",id:"step-1-create-a-custom-legged-robot-environment-in-gymnasium",level:3},{value:"Step 2: Write a Training Script Using Stable Baselines3 (PPO)",id:"step-2-write-a-training-script-using-stable-baselines3-ppo",level:3},{value:"Step 3: Update <code>setup.py</code> and <code>package.xml</code> for <code>my_rl_robot</code>",id:"step-3-update-setuppy-and-packagexml-for-my_rl_robot",level:3},{value:"Step 4: Build Your Package and Run Training",id:"step-4-build-your-package-and-run-training",level:3},{value:"Verification",id:"verification",level:2},{value:"Challenge Questions",id:"challenge-questions",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"lab-5-deep-reinforcement-learning-for-locomotion",children:"Lab 5: Deep Reinforcement Learning for Locomotion"})}),"\n",(0,i.jsx)(n.h2,{id:"objective",children:"Objective"}),"\n",(0,i.jsxs)(n.p,{children:["This lab introduces you to the exciting world of Deep Reinforcement Learning (DRL) for robot locomotion. You will set up a simplified simulated environment using ",(0,i.jsx)(n.code,{children:"Gymnasium"}),' and train a basic legged robot to "walk" (move forward) using the ',(0,i.jsx)(n.strong,{children:"Proximal Policy Optimization (PPO)"})," algorithm. This lab provides a conceptual foundation for applying DRL to more complex robots in environments like NVIDIA Isaac Lab."]}),"\n",(0,i.jsx)(n.h2,{id:"theoretical-background",children:"Theoretical Background"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Reinforcement Learning (RL)"}),", as discussed in ",(0,i.jsx)(n.strong,{children:"Chapter 7"}),", is about an agent learning to make sequential decisions by trial and error to maximize a cumulative reward."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Agent"}),": Our simulated legged robot."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environment"}),": The ",(0,i.jsx)(n.code,{children:"Gymnasium"})," simulation providing states and rewards."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State (Observation)"}),": Information about the robot (joint angles, velocities, IMU)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action"}),": Commands sent to the robot's joints (e.g., torques or target positions)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reward Function"}),': Defines what constitutes "good" behavior (e.g., moving forward, not falling).']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"PPO Algorithm"}),": A policy optimization algorithm robustly used for continuous control tasks in robotics."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Humble Development Environment"}),": Set up as in Lab 1 (though this lab is mostly Python-centric, the environment is suitable)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Python 3.10+"}),": With ",(0,i.jsx)(n.code,{children:"pip"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Install necessary libraries"}),":","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install gymnasium stable-baselines3[extra]\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"step-by-step-instructions",children:"Step-by-Step Instructions"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-create-a-custom-legged-robot-environment-in-gymnasium",children:"Step 1: Create a Custom Legged Robot Environment in Gymnasium"}),"\n",(0,i.jsx)(n.p,{children:"We will create a very simplified 2-DOF legged robot (like a single leg from a humanoid)."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Create a new Python file named ",(0,i.jsx)(n.code,{children:"~/ros2_ws/src/my_rl_robot/my_legged_env.py"})," (you might need to create the ",(0,i.jsx)(n.code,{children:"my_rl_robot"})," package first with ",(0,i.jsx)(n.code,{children:"ros2 pkg create --build-type ament_python my_rl_robot"}),")."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass SimpleLeggedRobotEnv(gym.Env):\n    """\n    A simplified Gymnasium environment for a 2-DOF legged robot.\n    The robot\'s goal is to move its end-effector to a target position.\n    """\n    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 30}\n\n    def __init__(self, render_mode=None):\n        super().__init__()\n        \n        # Action space: 2 joint position commands (e.g., hip and knee angle targets)\n        # Assuming joint angles are normalized between -1 and 1\n        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n\n        # Observation space: 2 joint angles, 2 joint velocities, 2 end-effector (x,y)\n        # Simplified: just joint positions and target\n        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)\n\n        # --- Robot State ---\n        self.joint_angles = np.array([0.0, 0.0], dtype=np.float32) # [hip, knee]\n        self.end_effector_pos = np.array([0.0, -0.5], dtype=np.float32) # Initial (x,y)\n        self.target_pos = np.array([0.5, -0.5], dtype=np.float32) # Go forward 0.5m\n\n        # --- Simulation Parameters ---\n        self.link_length = 0.5 # Both links are 0.5m long\n        self.dt = 0.05 # Time step\n\n        self.render_mode = render_mode\n        # self.window = None\n        # self.clock = None\n\n    def _get_obs(self):\n        # Observation = [current_hip_angle, current_knee_angle, target_x, target_y]\n        return np.concatenate((self.joint_angles, self.target_pos))\n\n    def _get_info(self):\n        return {"distance_to_target": np.linalg.norm(self.end_effector_pos - self.target_pos)}\n\n    def _forward_kinematics(self, hip_angle, knee_angle):\n        """Calculates the end-effector (x,y) position from joint angles."""\n        x = self.link_length * np.sin(hip_angle) + self.link_length * np.sin(hip_angle + knee_angle)\n        y = -self.link_length * np.cos(hip_angle) - self.link_length * np.cos(hip_angle + knee_angle)\n        return np.array([x, y])\n\n    def step(self, action):\n        # Convert action (normalized -1 to 1) to actual joint angle changes\n        # Simple PID-like action: target joint angles are adjusted by action\n        self.joint_angles += action * 0.1 # Small step towards target angle\n        self.joint_angles = np.clip(self.joint_angles, -np.pi/2, np.pi/2) # Joint limits -90 to 90 deg\n\n        # Update end-effector position\n        self.end_effector_pos = self._forward_kinematics(self.joint_angles[0], self.joint_angles[1])\n\n        # --- Reward Calculation ---\n        distance_to_target = np.linalg.norm(self.end_effector_pos - self.target_pos)\n        \n        # Reward for being close to target (negative distance)\n        reward = -distance_to_target \n\n        # Additional small reward for moving forward (increasing x)\n        reward += self.end_effector_pos[0] * 0.1\n        \n        # Penalty for excessive joint movement (energy)\n        reward -= np.sum(np.abs(action)) * 0.01\n\n        # --- Termination Condition ---\n        terminated = distance_to_target < 0.1 # Reached target\n        truncated = False # No time limit in this simple env\n\n        observation = self._get_obs()\n        info = self._get_info()\n\n        return observation, reward, terminated, truncated, info\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        \n        # Reset robot state\n        self.joint_angles = np.array([0.0, 0.0], dtype=np.float32)\n        self.end_effector_pos = self._forward_kinematics(self.joint_angles[0], self.joint_angles[1])\n        # Randomize target slightly for generalization\n        self.target_pos = np.array([0.5 + np.random.uniform(-0.1, 0.1), -0.5], dtype=np.float32) \n\n        observation = self._get_obs()\n        info = self._get_info()\n        return observation, info\n\n    def render(self):\n        # Basic rendering logic (can be expanded with Pygame/Matplotlib)\n        pass # Not implementing visual render for this lab\n\n    def close(self):\n        pass\n\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-write-a-training-script-using-stable-baselines3-ppo",children:"Step 2: Write a Training Script Using Stable Baselines3 (PPO)"}),"\n",(0,i.jsxs)(n.p,{children:["Create a new Python file named ",(0,i.jsx)(n.code,{children:"~/ros2_ws/src/my_rl_robot/train_legged_robot.py"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\nfrom my_legged_env import SimpleLeggedRobotEnv # Import our custom environment\n\nif __name__ == "__main__":\n    # 1. Create the environment\n    # For more complex robotics, this would be an Isaac Lab or Gazebo env\n    env = make_vec_env(SimpleLeggedRobotEnv, n_envs=4, seed=0) \n\n    # 2. Define the PPO agent\n    model = PPO(\n        "MlpPolicy", # Multi-layer Perceptron Policy\n        env,\n        verbose=1,\n        n_steps=2048, # Number of steps to run for each environment per update\n        batch_size=64, # Minibatch size\n        gamma=0.99, # Discount factor\n        gae_lambda=0.95, # Factor for Generalized Advantage Estimation\n        clip_range=0.2, # Clipping parameter for PPO\n        ent_coef=0.01, # Entropy coefficient for exploration\n        learning_rate=0.0003,\n        tensorboard_log="./ppo_legged_robot_log/" # For visualizing training progress\n    )\n\n    # 3. Define Callbacks for evaluation and early stopping\n    # Create a separate evaluation env\n    eval_env = SimpleLeggedRobotEnv()\n    stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improve_evals=3, min_evals=5, verbose=1)\n    eval_callback = EvalCallback(\n        eval_env, \n        callback_on_new_best=stop_train_callback, \n        best_model_save_path="./best_model_legged/",\n        log_path="./ppo_legged_robot_log/", \n        eval_freq=1000, \n        deterministic=True, \n        render=False\n    )\n\n    # 4. Train the agent\n    print("Starting training for Simple Legged Robot...")\n    model.learn(total_timesteps=50000, callback=eval_callback) # Train for 50,000 steps\n    print("Training finished.")\n\n    # 5. Save the final trained model\n    model.save("simple_legged_robot_policy")\n\n    # 6. (Optional) Test the trained agent\n    print("\\nTesting the trained agent...")\n    obs, info = eval_env.reset()\n    for i in range(100):\n        action, _states = model.predict(obs, deterministic=True)\n        obs, reward, terminated, truncated, info = eval_env.step(action)\n        # eval_env.render() # Uncomment if you implement rendering\n        if terminated or truncated:\n            obs, info = eval_env.reset()\n            print("Episode finished, resetting environment.")\n'})}),"\n",(0,i.jsxs)(n.h3,{id:"step-3-update-setuppy-and-packagexml-for-my_rl_robot",children:["Step 3: Update ",(0,i.jsx)(n.code,{children:"setup.py"})," and ",(0,i.jsx)(n.code,{children:"package.xml"})," for ",(0,i.jsx)(n.code,{children:"my_rl_robot"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsxs)(n.strong,{children:["Update ",(0,i.jsx)(n.code,{children:"~/ros2_ws/src/my_rl_robot/setup.py"})]}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from setuptools import setup\n\npackage_name = 'my_rl_robot'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=[package_name],\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='your_name',\n    maintainer_email='your_email@example.com',\n    description='TODO: Package description',\n    license='TODO: License declaration',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'train_legged_robot = my_rl_robot.train_legged_robot:main',\n        ],\n    },\n)\n\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsxs)(n.strong,{children:["Update ",(0,i.jsx)(n.code,{children:"~/ros2_ws/src/my_rl_robot/package.xml"})]}),":\nAdd these dependencies:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:"  <depend>python3-gymnasium</depend>\n  <depend>python3-stable-baselines3</depend>\n  <depend>python3-numpy</depend>\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-4-build-your-package-and-run-training",children:"Step 4: Build Your Package and Run Training"}),"\n",(0,i.jsx)(n.p,{children:"Navigate back to your workspace root and build your package:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws\ncolcon build --packages-select my_rl_robot\n"})}),"\n",(0,i.jsx)(n.p,{children:"Source your workspace:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:". install/setup.bash\n"})}),"\n",(0,i.jsx)(n.p,{children:"Now, run the training script:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 run my_rl_robot train_legged_robot\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Observe the training process in your terminal. You will see PPO reporting ",(0,i.jsx)(n.code,{children:"rollout/ep_len_mean"}),", ",(0,i.jsx)(n.code,{children:"rollout/ep_rew_mean"})," (episode reward mean), and ",(0,i.jsx)(n.code,{children:"time/fps"}),". The ",(0,i.jsx)(n.code,{children:"ep_rew_mean"})," should increase over time as the agent learns."]}),"\n",(0,i.jsx)(n.h2,{id:"verification",children:"Verification"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The training script runs without errors."}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"ep_rew_mean"})," in the training output shows a general increasing trend, indicating the agent is learning to achieve higher rewards (i.e., moving closer to the target)."]}),"\n",(0,i.jsxs)(n.li,{children:["A ",(0,i.jsx)(n.code,{children:"simple_legged_robot_policy.zip"})," file is saved, containing the trained model."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"challenge-questions",children:"Challenge Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reward Shaping"}),": Modify the ",(0,i.jsx)(n.code,{children:"_compute_reward"})," function in ",(0,i.jsx)(n.code,{children:"SimpleLeggedRobotEnv"}),". Add a penalty for joint angles being too close to their limits (e.g., ",(0,i.jsx)(n.code,{children:"-np.sum(np.clip(np.abs(self.joint_angles) - (np.pi/2 - 0.1), 0, None)) * 0.1"}),"). How does this affect the learning process and the final behavior?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Target Randomization"}),": In ",(0,i.jsx)(n.code,{children:"reset()"}),", randomize the ",(0,i.jsx)(n.code,{children:"target_pos"})," more broadly. Does the agent generalize better, or does it struggle to learn a single target?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hyperparameter Tuning"}),": Experiment with ",(0,i.jsx)(n.code,{children:"learning_rate"}),", ",(0,i.jsx)(n.code,{children:"gamma"}),", and ",(0,i.jsx)(n.code,{children:"clip_range"})," in the PPO model. How do these hyperparameters affect the speed and stability of learning?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Extend the Robot"}),": If you were to integrate this with a more realistic simulator like Isaac Lab, how would you map the ",(0,i.jsx)(n.code,{children:"step"})," and ",(0,i.jsx)(n.code,{children:"reset"})," functions of ",(0,i.jsx)(n.code,{children:"SimpleLeggedRobotEnv"})," to Isaac Lab's API?"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var o=t(6540);const i={},r=o.createContext(i);function a(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);