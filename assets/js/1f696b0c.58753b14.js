"use strict";(globalThis.webpackChunkcortex_h1=globalThis.webpackChunkcortex_h1||[]).push([[2166],{7509:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>a,contentTitle:()=>r,default:()=>u,frontMatter:()=>l,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/multimodal-fusion","title":"04. Multimodal Fusion Strategies","description":"To build a Vision-Language-Action (VLA) model, we must effectively combine information from different modalities (Vision, Text, Proprioception).","source":"@site/docs/module-4-vla/04-multimodal-fusion.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/multimodal-fusion","permalink":"/Cortex-H1/docs/module-4-vla/multimodal-fusion","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/04-multimodal-fusion.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"multimodal-fusion","title":"04. Multimodal Fusion Strategies","sidebar_label":"04. Multimodal Fusion"},"sidebar":"textbookSidebar","previous":{"title":"03. Vision Transformers","permalink":"/Cortex-H1/docs/module-4-vla/vision-transformers"},"next":{"title":"05. VLA Architectures","permalink":"/Cortex-H1/docs/module-4-vla/vla-architectures"}}');var o=n(4848),s=n(8453);const l={id:"multimodal-fusion",title:"04. Multimodal Fusion Strategies",sidebar_label:"04. Multimodal Fusion"},r="Multimodal Fusion Strategies",a={},c=[{value:"Fusion Architectures",id:"fusion-architectures",level:2},{value:"1. Early Fusion",id:"1-early-fusion",level:3},{value:"2. Late Fusion",id:"2-late-fusion",level:3},{value:"3. Cross-Attention Fusion",id:"3-cross-attention-fusion",level:3},{value:"Aligning Representations",id:"aligning-representations",level:2},{value:"Incorporation of Action",id:"incorporation-of-action",level:2}];function d(e){const i={em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(i.header,{children:(0,o.jsx)(i.h1,{id:"multimodal-fusion-strategies",children:"Multimodal Fusion Strategies"})}),"\n",(0,o.jsx)(i.p,{children:"To build a Vision-Language-Action (VLA) model, we must effectively combine information from different modalities (Vision, Text, Proprioception)."}),"\n",(0,o.jsx)(i.h2,{id:"fusion-architectures",children:"Fusion Architectures"}),"\n",(0,o.jsx)(i.h3,{id:"1-early-fusion",children:"1. Early Fusion"}),"\n",(0,o.jsx)(i.p,{children:"Concatenating raw features (e.g., image patches and text tokens) at the input level. The model processes them jointly from the start."}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.em,{children:"Pros"}),": Allows rich interaction between modalities at all levels."]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.em,{children:"Cons"}),": Computationally expensive; requires a unified architecture."]}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"2-late-fusion",children:"2. Late Fusion"}),"\n",(0,o.jsx)(i.p,{children:"Processing each modality with a separate encoder and combining them at the decision level."}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.em,{children:"Pros"}),": Modular; allows using pre-trained specialized encoders."]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.em,{children:"Cons"}),": Misses low-level correlations between modalities."]}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"3-cross-attention-fusion",children:"3. Cross-Attention Fusion"}),"\n",(0,o.jsx)(i.p,{children:"Using attention mechanisms to query information from one modality based on another. For example, using text instructions to attend to specific parts of an image."}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.em,{children:"Example"}),": Perceiver, Flamingo."]}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"aligning-representations",children:"Aligning Representations"}),"\n",(0,o.jsx)(i.p,{children:'Ensuring that "an apple" in text and an image of an apple map to compatible representations is critical. Contrastive learning (like in CLIP) is the standard approach for this alignment.'}),"\n",(0,o.jsx)(i.h2,{id:"incorporation-of-action",children:"Incorporation of Action"}),"\n",(0,o.jsx)(i.p,{children:'In VLA models, "Action" is treated as another modality. Action tokens (e.g., discretized joint velocities or end-effector poses) are predicted autoregressively alongside text or visual tokens.'})]})}function u(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,o.jsx)(i,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>l,x:()=>r});var t=n(6540);const o={},s=t.createContext(o);function l(e){const i=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),t.createElement(s.Provider,{value:i},e.children)}}}]);