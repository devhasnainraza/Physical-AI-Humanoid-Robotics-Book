"use strict";(globalThis.webpackChunkcortex_h1=globalThis.webpackChunkcortex_h1||[]).push([[7197],{2735:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-4-vla/vla-architectures","title":"05. State-of-the-Art VLA Architectures","description":"Vision-Language-Action (VLA) models represent the frontier of embodied AI, integrating perception, reasoning, and control into a single end-to-end system.","source":"@site/docs/module-4-vla/05-vla-architectures.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/vla-architectures","permalink":"/Cortex-H1/docs/module-4-vla/vla-architectures","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/05-vla-architectures.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"vla-architectures","title":"05. State-of-the-Art VLA Architectures","sidebar_label":"05. VLA Architectures"},"sidebar":"textbookSidebar","previous":{"title":"04. Multimodal Fusion","permalink":"/Cortex-H1/docs/module-4-vla/multimodal-fusion"},"next":{"title":"06. Future of VLA","permalink":"/Cortex-H1/docs/module-4-vla/future-of-vla"}}');var o=t(4848),s=t(8453);const i={id:"vla-architectures",title:"05. State-of-the-Art VLA Architectures",sidebar_label:"05. VLA Architectures"},a="State-of-the-Art VLA Architectures",l={},c=[{value:"RT-1 (Robotics Transformer 1)",id:"rt-1-robotics-transformer-1",level:2},{value:"RT-2 (Robotics Transformer 2)",id:"rt-2-robotics-transformer-2",level:2},{value:"PaLM-E",id:"palm-e",level:2},{value:"Octo",id:"octo",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"state-of-the-art-vla-architectures",children:"State-of-the-Art VLA Architectures"})}),"\n",(0,o.jsx)(n.p,{children:"Vision-Language-Action (VLA) models represent the frontier of embodied AI, integrating perception, reasoning, and control into a single end-to-end system."}),"\n",(0,o.jsx)(n.h2,{id:"rt-1-robotics-transformer-1",children:"RT-1 (Robotics Transformer 1)"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Google DeepMind"})}),"\n",(0,o.jsx)(n.p,{children:"RT-1 uses a ViT backbone for vision and tokenizes robot actions. It is trained on a large dataset of real-world robot demonstrations."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Architecture"}),": EfficientNet backbone + FiLM layers for conditioning on language + Token Learner + Transformer."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Output"}),": Discretized action tokens."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"rt-2-robotics-transformer-2",children:"RT-2 (Robotics Transformer 2)"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Google DeepMind"})}),"\n",(0,o.jsx)(n.p,{children:"RT-2 shows that large generic VLMs can be fine-tuned to output robot actions directly."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Approach"}),": Fine-tuning PaLI-X and PaLM-E models."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Key Insight"}),": The model retains its web-scale knowledge (reasoning, semantic understanding) while learning to control the robot, enabling generalization to unseen commands."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"palm-e",children:"PaLM-E"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Google"})}),"\n",(0,o.jsx)(n.p,{children:"An embodied multimodal language model."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Inputs"}),": Text, Images, State vectors (sensor data)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Outputs"}),": Text (planning) or control commands."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Mechanism"}),": Injects continuous sensor observations into the language model's embedding space."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"octo",children:"Octo"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"UC Berkeley / Stanford"})}),"\n",(0,o.jsx)(n.p,{children:"An open-source general-purpose robot policy."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Transformer-based"}),": Uses a diffusion head or categorical head for action prediction."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Training"}),": Trained on the Open X-Embodiment dataset (diverse robot data)."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>a});var r=t(6540);const o={},s=r.createContext(o);function i(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);