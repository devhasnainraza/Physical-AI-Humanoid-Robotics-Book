"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[313],{1443:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-5-advanced/robot-ethics","title":"Ethics & Safety in Physical AI","description":"1. The Alignment Problem","source":"@site/docs/module-5-advanced/07-robot-ethics.md","sourceDirName":"module-5-advanced","slug":"/module-5-advanced/robot-ethics","permalink":"/Cortex-H1/docs/module-5-advanced/robot-ethics","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-5-advanced/07-robot-ethics.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"title":"Ethics & Safety in Physical AI"},"sidebar":"textbookSidebar","previous":{"title":"Cloud Robotics with AWS RoboMaker","permalink":"/Cortex-H1/docs/module-5-advanced/cloud-robotics-aws"}}');var s=n(4848),i=n(8453);const r={sidebar_position:7,title:"Ethics & Safety in Physical AI"},a="Asimov's Laws in the Age of LLMs",l={},c=[{value:"1. The Alignment Problem",id:"1-the-alignment-problem",level:2},{value:"2. Constitutional AI for Robots",id:"2-constitutional-ai-for-robots",level:2},{value:"2.1 The Safety Filter Node",id:"21-the-safety-filter-node",level:3},{value:"3. The Trolley Problem in VSLAM",id:"3-the-trolley-problem-in-vslam",level:2}];function d(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"asimovs-laws-in-the-age-of-llms",children:"Asimov's Laws in the Age of LLMs"})}),"\n",(0,s.jsx)(t.h2,{id:"1-the-alignment-problem",children:"1. The Alignment Problem"}),"\n",(0,s.jsxs)(t.p,{children:['When an LLM controls a 50kg metal humanoid, "Hallucination" becomes "Physical Harm".\n',(0,s.jsx)(t.strong,{children:"Scenario"}),': User says "Make me a sandwich."\n',(0,s.jsx)(t.strong,{children:"Robot"}),": There is no knife.\n",(0,s.jsx)(t.strong,{children:"Un-aligned AI"}),': "I will break this glass bottle to create a cutting edge." -> ',(0,s.jsx)(t.strong,{children:"Catastrophic Failure"}),"."]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"2-constitutional-ai-for-robots",children:"2. Constitutional AI for Robots"}),"\n",(0,s.jsxs)(t.p,{children:["We cannot rely on RLHF (Chatbot training) alone. We need explicit ",(0,s.jsx)(t.strong,{children:"Safety Kernels"}),"."]}),"\n",(0,s.jsx)(t.h3,{id:"21-the-safety-filter-node",children:"2.1 The Safety Filter Node"}),"\n",(0,s.jsx)(t.p,{children:"A dedicated, non-AI node that vets every command from the LLM."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'def safety_check(action):\n    if action.type == "GRASP" and action.target == "Human Neck":\n        return False, "ETHICAL_VIOLATION"\n    \n    if action.velocity > MAX_SAFE_VELOCITY:\n        return False, "OVERSPEED"\n        \n    return True, "OK"\n'})}),"\n",(0,s.jsxs)(t.p,{children:['This node runs on the "Real-Time" layer, not the "Generative" layer. It is the ',(0,s.jsx)(t.strong,{children:"hard-coded conscience"})," of the robot."]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"3-the-trolley-problem-in-vslam",children:"3. The Trolley Problem in VSLAM"}),"\n",(0,s.jsx)(t.p,{children:"If a robot navigates a crowded hospital:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Path A: Fast, but passes close to elderly patients."}),"\n",(0,s.jsx)(t.li,{children:"Path B: Slow, safe detour."}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["A standard Nav2 costmap treats all obstacles equally.\n",(0,s.jsx)(t.strong,{children:"Social Navigation"}),' assigns "Social Costs" to humans.']}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Front of human: High Cost (Don't block them)."}),"\n",(0,s.jsx)(t.li,{children:"Back of human: Medium Cost (Don't startle them)."}),"\n",(0,s.jsx)(t.li,{children:"Group of humans: High Cost (Don't interrupt conversation)."}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:'Implementing these "Soft Laws" is the frontier of Social Robotics.'})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>a});var o=n(6540);const s={},i=o.createContext(s);function r(e){const t=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(i.Provider,{value:t},e.children)}}}]);