"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[101],{6404:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module-4-vla/intro","title":"Vision-Language-Action (VLA)","description":"1. The End of \\"Pipeline\\" AI?","source":"@site/docs/module-4-vla/intro.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/intro","permalink":"/Cortex-H1/docs/module-4-vla/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Vision-Language-Action (VLA)"},"sidebar":"textbookSidebar","previous":{"title":"Multi-Robot Fleet Management","permalink":"/Cortex-H1/docs/module-3-ai-brain/multi-robot-fleet"},"next":{"title":"Voice Command (Whisper)","permalink":"/Cortex-H1/docs/module-4-vla/whisper-integration"}}');var o=i(4848),t=i(8453);const r={sidebar_position:1,title:"Vision-Language-Action (VLA)"},l="Module 4: Vision-Language-Action Models",a={},d=[{value:"1. The End of &quot;Pipeline&quot; AI?",id:"1-the-end-of-pipeline-ai",level:2},{value:"2. The Practical VLA Pipeline (Today)",id:"2-the-practical-vla-pipeline-today",level:2},{value:"2.1 Why separate the LLM?",id:"21-why-separate-the-llm",level:3},{value:"3. Grounding: The Biggest Challenge",id:"3-grounding-the-biggest-challenge",level:2},{value:"4. Safety Guardrails",id:"4-safety-guardrails",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-vision-language-action-models",children:"Module 4: Vision-Language-Action Models"})}),"\n",(0,o.jsx)(n.h2,{id:"1-the-end-of-pipeline-ai",children:'1. The End of "Pipeline" AI?'}),"\n",(0,o.jsxs)(n.p,{children:["Traditionally, robotics used a modular pipeline:\n",(0,o.jsx)(n.code,{children:"Perception -> State Estimation -> Planning -> Control"})]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"VLA (Vision-Language-Action)"})," models attempt to fuse these into a single end-to-end model (or at least tightly coupled systems)."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Input"}),': "Put the red apple in the bowl" (Text) + Image of table (Vision).']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Output"}),": End-effector pose (x, y, z, grip) or joint velocities (Action)."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Examples: ",(0,o.jsx)(n.strong,{children:"Google RT-2"}),", ",(0,o.jsx)(n.strong,{children:"OpenVLA"}),"."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"2-the-practical-vla-pipeline-today",children:"2. The Practical VLA Pipeline (Today)"}),"\n",(0,o.jsxs)(n.p,{children:["Since end-to-end models are heavy and hard to control, we currently use a ",(0,o.jsx)(n.strong,{children:"Cognitive Pipeline"})," for production humanoids:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech-to-Text (Ear)"}),": OpenAI Whisper."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Planner (Brain)"}),": Large Language Model (GPT-4o, Gemini 1.5)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision (Eye)"}),": Open-Vocabulary Object Detector (OWL-ViT, Grounding DINO)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action (Body)"}),": Behavior Tree / Nav2."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"21-why-separate-the-llm",children:"2.1 Why separate the LLM?"}),"\n",(0,o.jsx)(n.p,{children:"LLMs are slow (tokens per second). Robots need 500Hz control loops."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"High Frequency Loop"}),": Joint Control (500Hz)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Medium Frequency Loop"}),": Path Planning (10Hz)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Low Frequency Loop"}),": LLM Reasoning (0.5Hz)."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:'The LLM is the "General," not the "Soldier." It gives orders; it doesn\'t pull the trigger.'}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"3-grounding-the-biggest-challenge",children:"3. Grounding: The Biggest Challenge"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Grounding"})," is the problem of connecting symbols to the physical world."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'LLM says: "Pick up the cup."'}),"\n",(0,o.jsx)(n.li,{children:'Robot asks: "Which set of pixels is the cup? Where is it in 3D space (x,y,z)?"'}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["We solve this with ",(0,o.jsx)(n.strong,{children:"V-LMs (Vision-Language Models)"})," or ",(0,o.jsx)(n.strong,{children:"Zero-Shot Detectors"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[User Audio] --\x3e|Whisper| B(Text: 'Get the Apple');\n    B --\x3e|Prompt| C{LLM Planner};\n    D[Camera Image] --\x3e E[Grounding DINO];\n    C --\x3e|Target: 'Apple'| E;\n    E --\x3e|Bounding Box| F[Depth Map];\n    F --\x3e|3D Coord (x,y,z)| G[Arm Controller];\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"4-safety-guardrails",children:"4. Safety Guardrails"}),"\n",(0,o.jsx)(n.p,{children:"LLMs hallucinate. A robot cannot hallucinate safely."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Hallucination"}),': "Jump over the building."']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Reality"}),": Robot tries to jump, falls over, breaks $10k hardware."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Guardrails"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feasibility Check"}),": Is the target coordinate reachable?"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety Policy"}),': Is the action listed in the "Allowed Actions" file?']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Human in the Loop"}),": Require confirmation for dangerous actions."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var s=i(6540);const o={},t=s.createContext(o);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);