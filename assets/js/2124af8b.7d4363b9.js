"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[965],{5305:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"module-4-vla/robot-transformers","title":"Robotic Transformers (RT-1, RT-2)","description":"1. The Image-Token Architecture","source":"@site/docs/module-4-vla/06-robot-transformers.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/robot-transformers","permalink":"/Cortex-H1/docs/module-4-vla/robot-transformers","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/06-robot-transformers.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"Robotic Transformers (RT-1, RT-2)"},"sidebar":"textbookSidebar","previous":{"title":"Fine-Tuning LLMs for Robotics","permalink":"/Cortex-H1/docs/module-4-vla/fine-tuning-llms"},"next":{"title":"Reinforcement Learning (Isaac Lab)","permalink":"/Cortex-H1/docs/module-5-advanced/reinforcement-learning"}}');var t=o(4848),i=o(8453);const s={sidebar_position:6,title:"Robotic Transformers (RT-1, RT-2)"},a="Google's Robotic Transformers",c={},l=[{value:"1. The Image-Token Architecture",id:"1-the-image-token-architecture",level:2},{value:"2. RT-2: Vision-Language-Action",id:"2-rt-2-vision-language-action",level:2},{value:"3. OpenVLA: The Open Source Alternative",id:"3-openvla-the-open-source-alternative",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"googles-robotic-transformers",children:"Google's Robotic Transformers"})}),"\n",(0,t.jsx)(n.h2,{id:"1-the-image-token-architecture",children:"1. The Image-Token Architecture"}),"\n",(0,t.jsxs)(n.p,{children:["Traditional VLA separates Vision and Language. ",(0,t.jsx)(n.strong,{children:"RT-1"}),' treats images as "tokens".\nThe input sequence is:\n',(0,t.jsx)(n.code,{children:'[Image Token 1] [Image Token 2] ... ["Pick up coke"]'})]}),"\n",(0,t.jsxs)(n.p,{children:["The output is ",(0,t.jsx)(n.strong,{children:"Action Tokens"}),":\n",(0,t.jsx)(n.code,{children:"[Arm x=12] [Arm y=45] [Gripper=Closed]"})]}),"\n",(0,t.jsx)(n.p,{children:'This allows the Transformer to "attend" to parts of the image just like it attends to words in a sentence.'}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"2-rt-2-vision-language-action",children:"2. RT-2: Vision-Language-Action"}),"\n",(0,t.jsxs)(n.p,{children:["RT-2 is fine-tuned from PaLI-X (a VLM).\nIt exhibits ",(0,t.jsx)(n.strong,{children:"Emergent Properties"}),"."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Train it on "Pick up apple".'}),"\n",(0,t.jsx)(n.li,{children:'Show it a picture of a "Darth Vader figure".'}),"\n",(0,t.jsx)(n.li,{children:'Command: "Pick up the dark lord."'}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Result"}),": It picks up Vader."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:'It transferred the semantic knowledge of "Dark Lord" from the web data to the robotic action space without explicit training.'}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"3-openvla-the-open-source-alternative",children:"3. OpenVLA: The Open Source Alternative"}),"\n",(0,t.jsxs)(n.p,{children:["Google's models are closed. ",(0,t.jsx)(n.strong,{children:"OpenVLA"})," is an open-source 7B model built on Llama 2 + SigLIP (Vision Encoder).\nYou can run it on a Jetson Orin Nano (quantized) to control a robot end-to-end."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Pipeline"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Camera -> Vision Encoder (SigLIP)."}),"\n",(0,t.jsx)(n.li,{children:"Text -> Tokenizer."}),"\n",(0,t.jsx)(n.li,{children:"Projector -> Fuses Vision embeddings into LLM space."}),"\n",(0,t.jsx)(n.li,{children:"LLM -> Predicts next action token (0-255 discretized joint value)."}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>a});var r=o(6540);const t={},i=r.createContext(t);function s(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);