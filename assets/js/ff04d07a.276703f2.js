"use strict";(globalThis.webpackChunkcortex_h1=globalThis.webpackChunkcortex_h1||[]).push([[8303],{8172:(s,e,n)=>{n.r(e),n.d(e,{assets:()=>d,contentTitle:()=>t,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-3-ai-brain/model-based-rl","title":"3.5 Model-Based RL","description":"Learning the physics of the world to plan: World Models and Dreamer.","source":"@site/docs/module-3-ai-brain/05-model-based-rl.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/model-based-rl","permalink":"/Cortex-H1/docs/module-3-ai-brain/model-based-rl","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-ai-brain/05-model-based-rl.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"model-based-rl","title":"3.5 Model-Based RL","sidebar_label":"3.5 Model-Based RL","description":"Learning the physics of the world to plan: World Models and Dreamer."},"sidebar":"textbookSidebar","previous":{"title":"3.4 Lab: PPO","permalink":"/Cortex-H1/docs/module-3-ai-brain/policy-gradients"},"next":{"title":"3.6 Lab: Sim-to-Real","permalink":"/Cortex-H1/docs/module-3-ai-brain/sim-to-real"}}');var l=n(4848),i=n(8453);const r={id:"model-based-rl",title:"3.5 Model-Based RL",sidebar_label:"3.5 Model-Based RL",description:"Learning the physics of the world to plan: World Models and Dreamer."},t="3.5 Model-Based RL",d={},c=[{value:"3.5.1 The World Model",id:"351-the-world-model",level:2},{value:"3.5.2 MPC with Learned Models (PETS)",id:"352-mpc-with-learned-models-pets",level:2},{value:"3.5.3 Dreamer (Learning inside Imagination)",id:"353-dreamer-learning-inside-imagination",level:2},{value:"3.5.4 Quiz",id:"354-quiz",level:2}];function m(s){const e={annotation:"annotation",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,i.R)(),...s.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(e.header,{children:(0,l.jsx)(e.h1,{id:"35-model-based-rl",children:"3.5 Model-Based RL"})}),"\n",(0,l.jsxs)(e.p,{children:["Model-Free RL (PPO, DQN) requires millions of samples because it learns by trial-and-error without understanding physics.\n",(0,l.jsx)(e.strong,{children:"Model-Based RL (MBRL)"})," learns a ",(0,l.jsx)(e.strong,{children:"World Model"})," first, then plans using that model."]}),"\n",(0,l.jsx)(e.h2,{id:"351-the-world-model",children:"3.5.1 The World Model"}),"\n",(0,l.jsxs)(e.p,{children:["We learn a function ",(0,l.jsxs)(e.span,{className:"katex",children:[(0,l.jsx)(e.span,{className:"katex-mathml",children:(0,l.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,l.jsxs)(e.semantics,{children:[(0,l.jsx)(e.mrow,{children:(0,l.jsx)(e.mi,{children:"f"})}),(0,l.jsx)(e.annotation,{encoding:"application/x-tex",children:"f"})]})})}),(0,l.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"0.8889em",verticalAlign:"-0.1944em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.10764em"},children:"f"})]})})]})," that mimics the environment:\n",(0,l.jsxs)(e.span,{className:"katex",children:[(0,l.jsx)(e.span,{className:"katex-mathml",children:(0,l.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,l.jsxs)(e.semantics,{children:[(0,l.jsxs)(e.mrow,{children:[(0,l.jsxs)(e.msub,{children:[(0,l.jsx)(e.mi,{children:"s"}),(0,l.jsxs)(e.mrow,{children:[(0,l.jsx)(e.mi,{children:"t"}),(0,l.jsx)(e.mo,{children:"+"}),(0,l.jsx)(e.mn,{children:"1"})]})]}),(0,l.jsx)(e.mo,{children:"="}),(0,l.jsx)(e.mi,{children:"f"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"("}),(0,l.jsxs)(e.msub,{children:[(0,l.jsx)(e.mi,{children:"s"}),(0,l.jsx)(e.mi,{children:"t"})]}),(0,l.jsx)(e.mo,{separator:"true",children:","}),(0,l.jsxs)(e.msub,{children:[(0,l.jsx)(e.mi,{children:"a"}),(0,l.jsx)(e.mi,{children:"t"})]}),(0,l.jsx)(e.mo,{stretchy:"false",children:")"})]}),(0,l.jsx)(e.annotation,{encoding:"application/x-tex",children:"s_{t+1} = f(s_t, a_t)"})]})})}),(0,l.jsxs)(e.span,{className:"katex-html","aria-hidden":"true",children:[(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"0.6389em",verticalAlign:"-0.2083em"}}),(0,l.jsxs)(e.span,{className:"mord",children:[(0,l.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,l.jsx)(e.span,{className:"msupsub",children:(0,l.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,l.jsxs)(e.span,{className:"vlist-r",children:[(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.3011em"},children:(0,l.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,l.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,l.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,l.jsxs)(e.span,{className:"mord mtight",children:[(0,l.jsx)(e.span,{className:"mord mathnormal mtight",children:"t"}),(0,l.jsx)(e.span,{className:"mbin mtight",children:"+"}),(0,l.jsx)(e.span,{className:"mord mtight",children:"1"})]})})]})}),(0,l.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,l.jsx)(e.span,{className:"vlist-r",children:(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.2083em"},children:(0,l.jsx)(e.span,{})})})]})})]}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,l.jsx)(e.span,{className:"mrel",children:"="}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.10764em"},children:"f"}),(0,l.jsx)(e.span,{className:"mopen",children:"("}),(0,l.jsxs)(e.span,{className:"mord",children:[(0,l.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,l.jsx)(e.span,{className:"msupsub",children:(0,l.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,l.jsxs)(e.span,{className:"vlist-r",children:[(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.2806em"},children:(0,l.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,l.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,l.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,l.jsx)(e.span,{className:"mord mathnormal mtight",children:"t"})})]})}),(0,l.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,l.jsx)(e.span,{className:"vlist-r",children:(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,l.jsx)(e.span,{})})})]})})]}),(0,l.jsx)(e.span,{className:"mpunct",children:","}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsxs)(e.span,{className:"mord",children:[(0,l.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,l.jsx)(e.span,{className:"msupsub",children:(0,l.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,l.jsxs)(e.span,{className:"vlist-r",children:[(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.2806em"},children:(0,l.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,l.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,l.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,l.jsx)(e.span,{className:"mord mathnormal mtight",children:"t"})})]})}),(0,l.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,l.jsx)(e.span,{className:"vlist-r",children:(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,l.jsx)(e.span,{})})})]})})]}),(0,l.jsx)(e.span,{className:"mclose",children:")"})]})]})]})]}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Pros"}),": Sample efficient. If we know the model, we can plan without moving the real robot (Safety)."]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Cons"}),": ",(0,l.jsx)(e.strong,{children:"Model Bias"}),'. If the model is wrong, the plan will be wrong. "Garbage in, garbage out."']}),"\n"]}),"\n",(0,l.jsx)(e.h2,{id:"352-mpc-with-learned-models-pets",children:"3.5.2 MPC with Learned Models (PETS)"}),"\n",(0,l.jsx)(e.p,{children:"Instead of using a predefined physics model (like in Ch 5.2), we use a Neural Network ensemble to predict dynamics."}),"\n",(0,l.jsxs)(e.ol,{children:["\n",(0,l.jsxs)(e.li,{children:["Sample ",(0,l.jsxs)(e.span,{className:"katex",children:[(0,l.jsx)(e.span,{className:"katex-mathml",children:(0,l.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,l.jsxs)(e.semantics,{children:[(0,l.jsx)(e.mrow,{children:(0,l.jsx)(e.mi,{children:"K"})}),(0,l.jsx)(e.annotation,{encoding:"application/x-tex",children:"K"})]})})}),(0,l.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"0.6833em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"},children:"K"})]})})]})," random action sequences."]}),"\n",(0,l.jsx)(e.li,{children:"Predict outcomes using the Neural Network model."}),"\n",(0,l.jsx)(e.li,{children:"Pick the best sequence."}),"\n",(0,l.jsx)(e.li,{children:"Execute first action."}),"\n"]}),"\n",(0,l.jsxs)(e.p,{children:[(0,l.jsx)(e.strong,{children:"Uncertainty Awareness"}),": By using an ",(0,l.jsx)(e.em,{children:"ensemble"})," of NNs, if they disagree, we know the model is uncertain. We can be cautious."]}),"\n",(0,l.jsx)(e.h2,{id:"353-dreamer-learning-inside-imagination",children:"3.5.3 Dreamer (Learning inside Imagination)"}),"\n",(0,l.jsxs)(e.p,{children:["State-of-the-art algorithms like ",(0,l.jsx)(e.strong,{children:"DreamerV3"})," learn a latent world model."]}),"\n",(0,l.jsxs)(e.ol,{children:["\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Encode"})," images into a compact latent state ",(0,l.jsxs)(e.span,{className:"katex",children:[(0,l.jsx)(e.span,{className:"katex-mathml",children:(0,l.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,l.jsxs)(e.semantics,{children:[(0,l.jsx)(e.mrow,{children:(0,l.jsx)(e.mi,{children:"z"})}),(0,l.jsx)(e.annotation,{encoding:"application/x-tex",children:"z"})]})})}),(0,l.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"0.4306em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.04398em"},children:"z"})]})})]}),"."]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Predict"})," future states in latent space (Recurrent Neural Network)."]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Train"}),' the Actor-Critic purely inside this "dream" (latent imagination).']}),"\n"]}),"\n",(0,l.jsx)(e.p,{children:'This allows robots to learn tasks in minutes of real time, as most training happens in the "dream" (GPU).'}),"\n",(0,l.jsx)(e.h2,{id:"354-quiz",children:"3.5.4 Quiz"}),"\n",(0,l.jsxs)(e.ol,{children:["\n",(0,l.jsxs)(e.li,{children:["\n",(0,l.jsx)(e.p,{children:(0,l.jsx)(e.strong,{children:"What is the main advantage of Model-Based RL?"})}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"a) It uses less compute."}),"\n",(0,l.jsx)(e.li,{children:"b) It is much more sample efficient (needs less data)."}),"\n",(0,l.jsx)(e.li,{children:"c) It is model-free."}),"\n",(0,l.jsx)(e.li,{children:(0,l.jsx)(e.em,{children:"Answer: b"})}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:["\n",(0,l.jsx)(e.p,{children:(0,l.jsx)(e.strong,{children:'What is "Model Bias"?'})}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"a) When the model prefers one color."}),"\n",(0,l.jsx)(e.li,{children:"b) Exploiting errors in the learned model, leading to failure in reality."}),"\n",(0,l.jsx)(e.li,{children:"c) A feature of fashion models."}),"\n",(0,l.jsx)(e.li,{children:(0,l.jsx)(e.em,{children:"Answer: b"})}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:["\n",(0,l.jsx)(e.p,{children:(0,l.jsx)(e.strong,{children:'How does "Dreamer" train?'})}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"a) While the robot is sleeping."}),"\n",(0,l.jsx)(e.li,{children:"b) By interacting with a learned latent dynamics model."}),"\n",(0,l.jsx)(e.li,{children:"c) Using real-world data only."}),"\n",(0,l.jsx)(e.li,{children:(0,l.jsx)(e.em,{children:"Answer: b"})}),"\n"]}),"\n"]}),"\n"]})]})}function h(s={}){const{wrapper:e}={...(0,i.R)(),...s.components};return e?(0,l.jsx)(e,{...s,children:(0,l.jsx)(m,{...s})}):m(s)}},8453:(s,e,n)=>{n.d(e,{R:()=>r,x:()=>t});var a=n(6540);const l={},i=a.createContext(l);function r(s){const e=a.useContext(i);return a.useMemo(function(){return"function"==typeof s?s(e):{...e,...s}},[e,s])}function t(s){let e;return e=s.disableParentContext?"function"==typeof s.components?s.components(l):s.components||l:r(s.components),a.createElement(i.Provider,{value:e},s.children)}}}]);