"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[958],{8169:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-3-ai-brain/visual-slam","title":"Visual SLAM","description":"1. The Localization Problem","source":"@site/docs/module-3-ai-brain/03-visual-slam.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/visual-slam","permalink":"/Cortex-H1/docs/module-3-ai-brain/visual-slam","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-ai-brain/03-visual-slam.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Visual SLAM"},"sidebar":"textbookSidebar","previous":{"title":"Nav2 Architecture","permalink":"/Cortex-H1/docs/module-3-ai-brain/navigation-stack"},"next":{"title":"Advanced Behavior Trees","permalink":"/Cortex-H1/docs/module-3-ai-brain/behavior-trees-deep-dive"}}');var t=n(4848),l=n(8453);const a={sidebar_position:3,title:"Visual SLAM"},r="Visual SLAM: Seeing and Mapping",o={},c=[{value:"1. The Localization Problem",id:"1-the-localization-problem",level:2},{value:"2. Visual vs LiDAR SLAM",id:"2-visual-vs-lidar-slam",level:2},{value:"3. Epipolar Geometry: The Math of Stereo Vision",id:"3-epipolar-geometry-the-math-of-stereo-vision",level:2},{value:"4. Using <code>isaac_ros_visual_slam</code>",id:"4-using-isaac_ros_visual_slam",level:2},{value:"4.1 Launching VSLAM",id:"41-launching-vslam",level:3},{value:"4.2 Loop Closure",id:"42-loop-closure",level:3}];function d(e){const i={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.header,{children:(0,t.jsx)(i.h1,{id:"visual-slam-seeing-and-mapping",children:"Visual SLAM: Seeing and Mapping"})}),"\n",(0,t.jsx)(i.h2,{id:"1-the-localization-problem",children:"1. The Localization Problem"}),"\n",(0,t.jsx)(i.p,{children:"Before a robot can navigate, it must answer two questions:"}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Mapping"}),": What does the world look like?"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Localization"}),": Where am I in that world?"]}),"\n"]}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"SLAM (Simultaneous Localization and Mapping)"})," solves both at once."]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.h2,{id:"2-visual-vs-lidar-slam",children:"2. Visual vs LiDAR SLAM"}),"\n",(0,t.jsxs)(i.table,{children:[(0,t.jsx)(i.thead,{children:(0,t.jsxs)(i.tr,{children:[(0,t.jsx)(i.th,{style:{textAlign:"left"},children:"Feature"}),(0,t.jsx)(i.th,{style:{textAlign:"left"},children:"LiDAR SLAM (e.g., Slam Toolbox)"}),(0,t.jsx)(i.th,{style:{textAlign:"left"},children:"Visual SLAM (e.g., ORB-SLAM3)"})]})}),(0,t.jsxs)(i.tbody,{children:[(0,t.jsxs)(i.tr,{children:[(0,t.jsx)(i.td,{style:{textAlign:"left"},children:(0,t.jsx)(i.strong,{children:"Input"})}),(0,t.jsx)(i.td,{style:{textAlign:"left"},children:"Laser Scan (2D/3D points)"}),(0,t.jsx)(i.td,{style:{textAlign:"left"},children:"Camera Images (Pixels)"})]}),(0,t.jsxs)(i.tr,{children:[(0,t.jsx)(i.td,{style:{textAlign:"left"},children:(0,t.jsx)(i.strong,{children:"Features"})}),(0,t.jsx)(i.td,{style:{textAlign:"left"},children:"Corners, Walls"}),(0,t.jsx)(i.td,{style:{textAlign:"left"},children:"Textures, Edges, Objects"})]}),(0,t.jsxs)(i.tr,{children:[(0,t.jsx)(i.td,{style:{textAlign:"left"},children:(0,t.jsx)(i.strong,{children:"Failure Mode"})}),(0,t.jsx)(i.td,{style:{textAlign:"left"},children:"Long corridors (geometric similarity)"}),(0,t.jsx)(i.td,{style:{textAlign:"left"},children:"Low light, motion blur, featureless white walls"})]}),(0,t.jsxs)(i.tr,{children:[(0,t.jsx)(i.td,{style:{textAlign:"left"},children:(0,t.jsx)(i.strong,{children:"Cost"})}),(0,t.jsx)(i.td,{style:{textAlign:"left"},children:"High (LiDARs are expensive)"}),(0,t.jsx)(i.td,{style:{textAlign:"left"},children:"Low (Cameras are cheap)"})]})]})]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.h2,{id:"3-epipolar-geometry-the-math-of-stereo-vision",children:"3. Epipolar Geometry: The Math of Stereo Vision"}),"\n",(0,t.jsx)(i.p,{children:"How do we get 3D depth ($Z$) from two 2D images?"}),"\n",(0,t.jsx)(i.p,{children:"Given two cameras with focal length $f$, separated by a baseline $b$, looking at a point $P$.\nThe disparity d is the difference in the x-coordinate of the point in the Left (xL) and Right (xR) images."}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.code,{children:"d = xL - xR"})}),"\n",(0,t.jsx)(i.p,{children:"The depth Z is calculated as:"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.code,{children:"Z = (f * b) / d"})}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Implication"}),":"]}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"If d is small (point is far away), depth estimation is noisy."}),"\n",(0,t.jsx)(i.li,{children:"If $b$ (baseline) is wider, depth accuracy improves at range, but close-up performance suffers."}),"\n"]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsxs)(i.h2,{id:"4-using-isaac_ros_visual_slam",children:["4. Using ",(0,t.jsx)(i.code,{children:"isaac_ros_visual_slam"})]}),"\n",(0,t.jsx)(i.p,{children:"NVIDIA provides a highly optimized VSLAM package for the Jetson."}),"\n",(0,t.jsx)(i.h3,{id:"41-launching-vslam",children:"4.1 Launching VSLAM"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\ndef generate_launch_description():\n    visual_slam_node = ComposableNode(\n        name='visual_slam_node',\n        package='isaac_ros_visual_slam',\n        plugin='nvidia::isaac_ros::visual_slam::VisualSlamNode',\n        parameters=[{\n            'enable_imu_fusion': True, # Use gyroscope for better stability\n            'enable_rectified_pose': True\n        }],\n        remappings=[\n            ('stereo_camera/left/image_rect', '/camera/left/image_rect'),\n            ('stereo_camera/right/image_rect', '/camera/right/image_rect'),\n            ('visual_slam/imu', '/camera/imu')\n        ]\n    )\n\n    return LaunchDescription([\n        ComposableNodeContainer(\n            name='visual_slam_container',\n            namespace='',\n            package='rclcpp_components',\n            executable='component_container',\n            composable_node_descriptions=[visual_slam_node],\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,t.jsx)(i.h3,{id:"42-loop-closure",children:"4.2 Loop Closure"}),"\n",(0,t.jsxs)(i.p,{children:["When the robot revisits a known location, accumulated drift error (Dead Reckoning) is corrected.\nMathematically, this is a ",(0,t.jsx)(i.strong,{children:"Pose Graph Optimization"})," problem.\nWe minimize the error:"]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.code,{children:"E = sum( (z_i - h(x_i))^2 )"})}),"\n",(0,t.jsx)(i.p,{children:"Where z_i is the sensor measurement and x_i is the estimated pose."})]})}function h(e={}){const{wrapper:i}={...(0,l.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>r});var s=n(6540);const t={},l=s.createContext(t);function a(e){const i=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(l.Provider,{value:i},e.children)}}}]);