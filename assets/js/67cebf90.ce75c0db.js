"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[8049],{1080:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4-vla/whisper-integration","title":"Voice Command (Whisper)","description":"1. Setting up the Ear","source":"@site/docs/module-4-vla/02-whisper-integration.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/whisper-integration","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/whisper-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/02-whisper-integration.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Voice Command (Whisper)"},"sidebar":"textbookSidebar","previous":{"title":"Vision-Language-Action (VLA)","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/intro"},"next":{"title":"The LLM Planner","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/llm-planner"}}');var i=t(4848),r=t(8453);const s={sidebar_position:2,title:"Voice Command (Whisper)"},a="Voice-to-Action with OpenAI Whisper",d={},l=[{value:"1. Setting up the Ear",id:"1-setting-up-the-ear",level:2},{value:"1.1 Python Audio Loop",id:"11-python-audio-loop",level:3},{value:"2. Faster Whisper (On-Edge)",id:"2-faster-whisper-on-edge",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"voice-to-action-with-openai-whisper",children:"Voice-to-Action with OpenAI Whisper"})}),"\n",(0,i.jsx)(n.h2,{id:"1-setting-up-the-ear",children:"1. Setting up the Ear"}),"\n",(0,i.jsxs)(n.p,{children:["To talk to our robot, we need a microphone array (like ReSpeaker) and a transcription engine. ",(0,i.jsx)(n.strong,{children:"OpenAI Whisper"})," is the state-of-the-art for robustness against noise."]}),"\n",(0,i.jsx)(n.h3,{id:"11-python-audio-loop",children:"1.1 Python Audio Loop"}),"\n",(0,i.jsx)(n.p,{children:'We need a script that listens for a "Wake Word" (optional) or records on button press, then sends audio to the API.'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import os\nimport openai\nimport sounddevice as sd\nimport soundfile as sf\nimport numpy as np\n\n# Configure\nopenai.api_key = os.getenv("OPENAI_API_KEY")\nSAMPLE_RATE = 44100\nDURATION = 5 # Record for 5 seconds\n\ndef record_audio(filename="command.wav"):\n    print("Listening... Speak now.")\n    audio_data = sd.rec(int(DURATION * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1)\n    sd.wait()\n    print("Processing...")\n    sf.write(filename, audio_data, SAMPLE_RATE)\n    return filename\n\ndef transcribe(filename):\n    with open(filename, "rb") as audio_file:\n        transcript = openai.audio.transcriptions.create(\n            model="whisper-1", \n            file=audio_file\n        )\n    return transcript.text\n\nif __name__ == "__main__":\n    file = record_audio()\n    text = transcribe(file)\n    print(f"User said: \'{text}\'")\n    # Next: Send \'text\' to the LLM Planner\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"2-faster-whisper-on-edge",children:"2. Faster Whisper (On-Edge)"}),"\n",(0,i.jsxs)(n.p,{children:["Running OpenAI API introduces latency (and cost). For a robot, we often run ",(0,i.jsx)(n.strong,{children:"Faster-Whisper"})," locally on the Jetson Orin GPU."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install faster-whisper\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from faster_whisper import WhisperModel\n\nmodel_size = "medium.en"\n# Run on GPU with FP16\nmodel = WhisperModel(model_size, device="cuda", compute_type="float16")\n\nsegments, info = model.transcribe("command.wav", beam_size=5)\n\nfor segment in segments:\n    print("[%.2fs -> %.2fs] %s" % (segment.start, segment.end, segment.text))\n'})}),"\n",(0,i.jsx)(n.p,{children:"This reduces latency from ~2s (Cloud) to ~300ms (Edge), making the interaction feel natural."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var o=t(6540);const i={},r=o.createContext(i);function s(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);