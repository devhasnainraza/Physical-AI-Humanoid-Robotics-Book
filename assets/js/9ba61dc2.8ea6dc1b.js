"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[459],{724:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-5-advanced/reinforcement-learning","title":"Reinforcement Learning (Isaac Lab)","description":"1. The Classical vs Learning Approach","source":"@site/docs/module-5-advanced/01-reinforcement-learning.md","sourceDirName":"module-5-advanced","slug":"/module-5-advanced/reinforcement-learning","permalink":"/Cortex-H1/docs/module-5-advanced/reinforcement-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-5-advanced/01-reinforcement-learning.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Reinforcement Learning (Isaac Lab)"},"sidebar":"textbookSidebar","previous":{"title":"Robotic Transformers (RT-1, RT-2)","permalink":"/Cortex-H1/docs/module-4-vla/robot-transformers"},"next":{"title":"CI/CD for Robotics","permalink":"/Cortex-H1/docs/module-5-advanced/cicd-for-robotics"}}');var i=r(4848),o=r(8453);const s={sidebar_position:1,title:"Reinforcement Learning (Isaac Lab)"},a="Deep Reinforcement Learning for Locomotion",l={},c=[{value:"1. The Classical vs Learning Approach",id:"1-the-classical-vs-learning-approach",level:2},{value:"2. The PPO Algorithm",id:"2-the-ppo-algorithm",level:2},{value:"2.1 The Reward Function",id:"21-the-reward-function",level:3},{value:"3. Training Pipeline",id:"3-training-pipeline",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"deep-reinforcement-learning-for-locomotion",children:"Deep Reinforcement Learning for Locomotion"})}),"\n",(0,i.jsx)(n.h2,{id:"1-the-classical-vs-learning-approach",children:"1. The Classical vs Learning Approach"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Classical Control"}),": You write the math equations for walking (ZMP, LIPM). It is stable but hard to tune for uneven terrain."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Deep RL"}),': You tell the robot "Don\'t fall" (Reward Function) and let it try 10 million times in simulation.']}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"NVIDIA Isaac Lab"})," (built on Isaac Sim) allows us to simulate these 10 million steps in minutes by running the physics on the GPU."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"2-the-ppo-algorithm",children:"2. The PPO Algorithm"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Proximal Policy Optimization (PPO)"})," is the standard for robotics."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Actor Neural Net"}),": Takes State (joint positions, IMU) -> Outputs Action (joint targets)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Critic Neural Net"}),': Estimates "How good is this state?"']}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"21-the-reward-function",children:"2.1 The Reward Function"}),"\n",(0,i.jsx)(n.p,{children:'Writing the reward function is the "Art" of RL.'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def compute_rewards(self):\n    # 1. Survival Reward (Don't fall)\n    reward = 1.0\n    \n    # 2. Velocity Tracking (Go target speed)\n    lin_vel_error = torch.square(self.commands[:, :2] - self.base_lin_vel[:, :2])\n    reward -= 1.0 * torch.exp(-lin_vel_error / 0.25)\n    \n    # 3. Energy Penalty (Don't be jittery)\n    reward -= 0.01 * torch.square(self.torques).sum(dim=1)\n    \n    # 4. Foot Air Time (Lift feet high)\n    reward += 0.5 * self.feet_air_time\n    \n    return reward\n"})}),"\n",(0,i.jsx)(n.p,{children:'If you forget the "Energy Penalty", the robot might vibrate its motors at 100Hz to stay upright!'}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"3-training-pipeline",children:"3. Training Pipeline"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Randomization"}),": Randomize friction (0.5-1.5), payload mass (+5kg), and push the robot randomly."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Training"}),": Run for 10,000 iterations."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sim-to-Real"}),": Export the Policy (",(0,i.jsx)(n.code,{children:"policy.onnx"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Deployment"}),": Run the ONNX model on the Jetson Orin."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The beauty of RL is that the final policy is just a small neural network (e.g., 256x128x64) that runs incredibly fast (microseconds)."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var t=r(6540);const i={},o=t.createContext(i);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);