"use strict";(globalThis.webpackChunkcortex_h1=globalThis.webpackChunkcortex_h1||[]).push([[8177],{102:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"labs/lab-6-llm-planner","title":"Lab 6: Implementing an LLM Planner for Robotic Actions","description":"Objective","source":"@site/docs/labs/lab-6-llm-planner.md","sourceDirName":"labs","slug":"/labs/lab-6-llm-planner","permalink":"/Cortex-H1/docs/labs/lab-6-llm-planner","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/labs/lab-6-llm-planner.md","tags":[],"version":"current","frontMatter":{"id":"lab-6-llm-planner","title":"Lab 6: Implementing an LLM Planner for Robotic Actions","sidebar_label":"Lab 6: LLM Planner"}}');var a=t(4848),s=t(8453);const r={id:"lab-6-llm-planner",title:"Lab 6: Implementing an LLM Planner for Robotic Actions",sidebar_label:"Lab 6: LLM Planner"},i="Lab 6: Implementing an LLM Planner for Robotic Actions",l={},c=[{value:"Objective",id:"objective",level:2},{value:"Theoretical Background",id:"theoretical-background",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step-by-Step Instructions",id:"step-by-step-instructions",level:2},{value:"Step 1: Define Robot Actions (Tools)",id:"step-1-define-robot-actions-tools",level:3},{value:"Step 2: Implement the LLM Planner with ReAct Strategy",id:"step-2-implement-the-llm-planner-with-react-strategy",level:3},{value:"Step 3: Update <code>setup.py</code> and <code>package.xml</code> for <code>my_llm_planner</code>",id:"step-3-update-setuppy-and-packagexml-for-my_llm_planner",level:3},{value:"Step 4: Build Your Package and Run the Planner",id:"step-4-build-your-package-and-run-the-planner",level:3},{value:"Verification",id:"verification",level:2},{value:"Challenge Questions",id:"challenge-questions",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"lab-6-implementing-an-llm-planner-for-robotic-actions",children:"Lab 6: Implementing an LLM Planner for Robotic Actions"})}),"\n",(0,a.jsx)(n.h2,{id:"objective",children:"Objective"}),"\n",(0,a.jsxs)(n.p,{children:["This lab brings together concepts from ",(0,a.jsx)(n.strong,{children:"Chapter 6: Planning and Decision Making"})," and ",(0,a.jsx)(n.strong,{children:"Chapter 11: Future of Physical AI"}),'. You will implement an LLM-based planner that interprets high-level human commands (e.g., "Go to the kitchen and fetch a soda") and translates them into a sequence of low-level robot actions. We will use the ',(0,a.jsx)(n.strong,{children:"ReAct (Reason + Act)"})," prompting strategy to make the planner robust."]}),"\n",(0,a.jsx)(n.h2,{id:"theoretical-background",children:"Theoretical Background"}),"\n",(0,a.jsx)(n.p,{children:"Traditional robotics planning relies on predefined state machines or planning algorithms (like A*). LLM planners offer a more flexible, human-like way to interpret complex commands."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"ReAct Prompting"}),": A strategy where the LLM interleaves ",(0,a.jsx)(n.strong,{children:"Thought"})," (internal monologue) and ",(0,a.jsx)(n.strong,{children:"Action"})," (calling predefined tools/functions). This allows the LLM to reason, execute actions, observe their outcomes, and iteratively refine its plan, similar to how humans solve problems."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Tool Use / Function Calling"}),': Providing the LLM with a list of available robot "skills" (functions) it can call, along with their descriptions and parameters. The LLM\'s task is then to choose and sequence these tools.']}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Python 3.10+"}),": With ",(0,a.jsx)(n.code,{children:"pip"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"OpenAI API Key"}),": Or access to Gemini API / Anthropic API. (We'll use OpenAI for this lab). Set ",(0,a.jsx)(n.code,{children:"OPENAI_API_KEY"})," environment variable.","\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'export OPENAI_API_KEY="YOUR_API_KEY"\n'})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Install necessary libraries"}),":","\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install openai # For OpenAI's Python client\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"step-by-step-instructions",children:"Step-by-Step Instructions"}),"\n",(0,a.jsx)(n.h3,{id:"step-1-define-robot-actions-tools",children:"Step 1: Define Robot Actions (Tools)"}),"\n",(0,a.jsx)(n.p,{children:'First, we define the "tools" our robot has. In a real ROS 2 system, these would map to ROS 2 actions or services. For this lab, we\'ll represent them as Python functions that just print what the robot would do.'}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Create a new Python file named ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/my_llm_planner/robot_actions.py"})," (you might need to create the ",(0,a.jsx)(n.code,{children:"my_llm_planner"})," package first with ",(0,a.jsx)(n.code,{children:"ros2 pkg create --build-type ament_python my_llm_planner"}),")."]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# ~/ros2_ws/src/my_llm_planner/robot_actions.py\n\ndef navigate_to(location: str):\n    """\n    Moves the robot to a specified named location.\n    Args:\n        location (str): The name of the location (e.g., "kitchen", "bedroom", "charging_station").\n    """\n    print(f"[ROBOT_ACTION] Navigating to: {location}")\n    # In a real system, this would call a ROS 2 Nav2 action client\n    return f"Successfully navigated to {location}"\n\ndef find_object(object_name: str):\n    """\n    Rotates the robot\'s head/camera to look for a specified object.\n    Args:\n        object_name (str): The name of the object to find (e.g., "soda can", "keys").\n    Returns:\n        str: JSON string with object\'s 3D coordinates if found, else "Object not found".\n    """\n    print(f"[ROBOT_ACTION] Looking for: {object_name}")\n    # Simulate finding the object\n    if object_name == "soda can":\n        return \'{{"object_name": "soda can", "x": 0.5, "y": 0.2, "z": 0.8}}\'\n    else:\n        return \'Object not found\'\n\ndef pick_up(x: float, y: float, z: float):\n    """\n    Commands the robot arm to pick up an object at given 3D coordinates.\n    Args:\n        x (float): X coordinate of the object relative to the robot base.\n        y (float): Y coordinate of the object relative to the robot base.\n        z (float): Z coordinate of the object relative to the robot base.\n    """\n    print(f"[ROBOT_ACTION] Picking up object at ({x:.2f}, {y:.2f}, {z:.2f})")\n    # Simulate picking up\n    return "Object picked up successfully"\n\ndef speak(text: str):\n    """\n    Makes the robot speak a given text aloud.\n    Args:\n        text (str): The text for the robot to say.\n    """\n    print(f"[ROBOT_ACTION] Robot says: \'{text}\'")\n    # In a real system, this would use a Text-to-Speech service\n    return "Spoke successfully"\n\n# Map tool names to actual functions\nTOOLS = {\n    "navigate_to": navigate_to,\n    "find_object": find_object,\n    "pick_up": pick_up,\n    "speak": speak\n}\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-2-implement-the-llm-planner-with-react-strategy",children:"Step 2: Implement the LLM Planner with ReAct Strategy"}),"\n",(0,a.jsx)(n.p,{children:"Now we create the main planner that uses these tools."}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Create a file named ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/my_llm_planner/llm_planner_node.py"})," and add the following Python code:"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os\nimport json\nimport openai\nfrom my_llm_planner.robot_actions import TOOLS # Import our tools\n\n# Set up OpenAI API key\nopenai.api_key = os.getenv("OPENAI_API_KEY")\n\nclass LLMPlanner:\n    def __init__(self):\n        self.model_name = "gpt-4-turbo" # Or "gpt-3.5-turbo", "gpt-4" for cheaper/faster\n        self.client = openai.OpenAI()\n\n    def generate_react_prompt(self, user_command, available_tools):\n        """Generates a ReAct-style prompt for the LLM."""\n        tool_descriptions = "\\n".join([\n            f"- {name}({\', \'.join(f\'{k}: {v.__annotations__[k].__name__}\' for k, v in tool.__annotations__.items() if k != \'return\')}) -> {tool.__doc__.strip()}"\n            for name, tool in available_tools.items()\n        ])\n\n        system_message = f"""\n        You are a highly intelligent robot planner named Alfred. Your goal is to translate high-level human commands into a sequence of executable robot actions.\n\n        You have access to the following tools:\n        {tool_descriptions}\n\n        To use a tool, respond in this exact JSON format:\n        {{\n            "action": "tool_name",\n            "args": {{"param1": "value1", "param2": "value2"}}\n        }}\n\n        You should use the following format for your responses:\n        Thought: You should always think about what to do.\n        Action: {{...}} (tool call in JSON)\n        Observation: The result of the action.\n        ... (this Thought/Action/Observation cycle repeats)\n        Thought: I have completed the task.\n        Action: "task_complete"\n\n        Begin!\n\n        Example:\n        User: Move to the bedroom.\n        Thought: The user wants me to navigate to the bedroom. I should use the navigate_to tool.\n        Action: {{"action": "navigate_to", "args": {{"location": "bedroom"}}}} \n        Observation: Successfully navigated to bedroom\n        Thought: I have completed the task.\n        Action: "task_complete"\n        """\n        \n        return system_message\n\n    def plan(self, user_command, max_iterations=5):\n        """\n        Generates and executes a plan for the robot using the LLM and available tools.\n        """\n        messages = [\n            {"role": "system", "content": self.generate_react_prompt(user_command, TOOLS)},\n            {"role": "user", "content": user_command}\n        ]\n        \n        for i in range(max_iterations):\n            print(f"\\n--- Iteration {i+1} ---")\n            \n            # Get LLM response\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=messages,\n                stop=["Observation:"] # Stop when LLM is about to output an observation\n            )\n            \n            llm_output = response.choices[0].message.content\n            print(f"LLM Output:\\n{llm_output}")\n            messages.append({"role": "assistant", "content": llm_output})\n\n            # Extract Thought and Action\n            thought_prefix = "Thought:"\n            action_prefix = "Action:"\n            \n            thought = ""\n            action_json_str = ""\n\n            # Parse thought and action\n            if thought_prefix in llm_output:\n                thought_start = llm_output.find(thought_prefix) + len(thought_prefix)\n                thought_end = llm_output.find(action_prefix, thought_start)\n                if thought_end != -1:\n                    thought = llm_output[thought_start:thought_end].strip()\n                else:\n                    thought = llm_output[thought_start:].strip()\n            \n            if action_prefix in llm_output:\n                action_start = llm_output.find(action_prefix) + len(action_prefix)\n                action_json_str = llm_output[action_start:].strip()\n\n            if not action_json_str:\n                print("Error: Could not parse action JSON from LLM output.")\n                messages.append({"role": "user", "content": "Error: Could not parse action. Please provide valid JSON action."})\n                continue\n            \n            try:\n                action_data = json.loads(action_json_str)\n            except json.JSONDecodeError:\n                print(f"Error decoding JSON: {action_json_str}")\n                messages.append({"role": "user", "content": f"Error: Invalid JSON action: {action_json_str}. Please provide valid JSON."})\n                continue\n\n            # Execute action\n            if action_data == "task_complete":\n                print("[PLANNER] Task completed by LLM.")\n                break\n            \n            tool_name = action_data.get("action")\n            tool_args = action_data.get("args", {})\n            \n            if tool_name in TOOLS:\n                print(f"[PLANNER] Executing tool: {tool_name} with args: {tool_args}")\n                observation = TOOLS[tool_name](**tool_args)\n            else:\n                observation = f"Error: Tool \'{tool_name}\' not found."\n            \n            print(f"Observation: {observation}")\n            messages.append({"role": "user", "content": f"Observation: {observation}"})\n            \n        else:\n            print("[PLANNER] Max iterations reached, task not completed.")\n        \n        return messages\n\n\ndef main():\n    planner = LLMPlanner()\n    \n    while True:\n        user_command = input("\\nUser Command (type \'exit\' to quit): ")\n        if user_command.lower() == \'exit\':\n            break\n        \n        planner.plan(user_command)\n\nif __name__ == \'__main__\':\n    main()\n\n'})}),"\n",(0,a.jsxs)(n.h3,{id:"step-3-update-setuppy-and-packagexml-for-my_llm_planner",children:["Step 3: Update ",(0,a.jsx)(n.code,{children:"setup.py"})," and ",(0,a.jsx)(n.code,{children:"package.xml"})," for ",(0,a.jsx)(n.code,{children:"my_llm_planner"})]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.strong,{children:["Update ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/my_llm_planner/setup.py"})]}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from setuptools import setup\n\npackage_name = 'my_llm_planner'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=[package_name],\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='your_name',\n    maintainer_email='your_email@example.com',\n    description='TODO: Package description',\n    license='TODO: License declaration',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'llm_planner = my_llm_planner.llm_planner_node:main',\n        ],\n    },\n)\n\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsxs)(n.strong,{children:["Update ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/my_llm_planner/package.xml"})]}),":\nAdd these dependencies:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"  <depend>python3-openai</depend>\n  \x3c!-- Other dependencies if you were to integrate with ROS 2 --\x3e\n"})}),"\n",(0,a.jsx)(n.h3,{id:"step-4-build-your-package-and-run-the-planner",children:"Step 4: Build Your Package and Run the Planner"}),"\n",(0,a.jsx)(n.p,{children:"Navigate back to your workspace root and build your package:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws\ncolcon build --packages-select my_llm_planner\n"})}),"\n",(0,a.jsx)(n.p,{children:"Source your workspace:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:". install/setup.bash\n"})}),"\n",(0,a.jsx)(n.p,{children:"Now, run your LLM planner:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ros2 run my_llm_planner llm_planner\n"})}),"\n",(0,a.jsx)(n.p,{children:"The script will prompt you for commands. Try:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"Go to the kitchen."})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"Find the soda can in the kitchen."})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"Pick up the soda can."})}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"verification",children:"Verification"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"The script successfully starts and prompts for input."}),"\n",(0,a.jsxs)(n.li,{children:["The LLM generates ",(0,a.jsx)(n.code,{children:"Thought"})," and ",(0,a.jsx)(n.code,{children:"Action"})," steps in response to your command."]}),"\n",(0,a.jsxs)(n.li,{children:["The simulated ",(0,a.jsx)(n.code,{children:"robot_actions.py"})," functions are called and print their output."]}),"\n",(0,a.jsxs)(n.li,{children:["The LLM continues to plan based on the ",(0,a.jsx)(n.code,{children:"Observation"}),' from the executed actions until "task_complete".']}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"challenge-questions",children:"Challenge Questions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Error Handling"}),": Modify the ",(0,a.jsx)(n.code,{children:"find_object"}),' function to sometimes return "Object not found". How does the LLM react? Can you improve the LLM\'s system prompt to better handle such failures (e.g., "If an object is not found, try navigating to another common location for it")?']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"New Tool"}),": Add a new tool, ",(0,a.jsx)(n.code,{children:"charge_battery()"}),", to ",(0,a.jsx)(n.code,{children:"robot_actions.py"}),' and describe it in the prompt. Test if the LLM can use this new tool appropriately when given a command like "Charge yourself."']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-time Integration"}),": Discuss how you would integrate this Python planner with a live ROS 2 system. How would ",(0,a.jsx)(n.code,{children:"robot_actions.py"})," functions interact with ROS 2 services and actions? (Hint: You would create ROS 2 action clients/service clients within ",(0,a.jsx)(n.code,{children:"robot_actions.py"}),")."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Observation Feedback"}),": Instead of just passing back ",(0,a.jsx)(n.code,{children:"Successfully navigated"}),', pass more detailed observations (e.g., "Navigated to kitchen, but path was obstructed once"). How does this richer observation affect the LLM\'s subsequent planning?']}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>i});var o=t(6540);const a={},s=o.createContext(a);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);