"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[101],{6404:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4-vla/intro","title":"Vision-Language-Action","description":"The Final Frontier: Conversational Robotics","source":"@site/docs/module-4-vla/intro.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/intro","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Vision-Language-Action"},"sidebar":"textbookSidebar","previous":{"title":"The AI-Robot Brain","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3-ai-brain/intro"}}');var t=o(4848),s=o(8453);const a={sidebar_position:1,title:"Vision-Language-Action"},r="Module 4: Vision-Language-Action (VLA)",l={},c=[{value:"The Final Frontier: Conversational Robotics",id:"the-final-frontier-conversational-robotics",level:2},{value:"Voice-to-Action Pipeline",id:"voice-to-action-pipeline",level:3},{value:"LLM-Based Cognitive Planning",id:"llm-based-cognitive-planning",level:3},{value:"Capstone: Autonomous Humanoid",id:"capstone-autonomous-humanoid",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.h2,{id:"the-final-frontier-conversational-robotics",children:"The Final Frontier: Conversational Robotics"}),"\n",(0,t.jsxs)(n.p,{children:["A ",(0,t.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," model connects natural language (what we say) and vision (what the robot sees) directly to robotic actions (what the robot does)."]}),"\n",(0,t.jsx)(n.h3,{id:"voice-to-action-pipeline",children:"Voice-to-Action Pipeline"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hear"}),": Use ",(0,t.jsx)(n.strong,{children:"OpenAI Whisper"}),' to transcribe voice commands ("Go pick up the red ball").']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Think"}),": An LLM (like GPT-4o or Gemini 1.5 Pro) plans the task.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Input"}),': "Pick up red ball" + [List of visible objects]']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Output"}),": ",(0,t.jsx)(n.code,{children:'[{"action": "navigate", "target": "red_ball"}, {"action": "grasp"}]'})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Act"}),": The robot executes the primitive actions."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"llm-based-cognitive-planning",children:"LLM-Based Cognitive Planning"}),"\n",(0,t.jsx)(n.p,{children:'We treat the LLM as the "Prefrontal Cortex" of the robot. It handles high-level reasoning, while ROS 2 handles the muscle memory.'}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"graph LR\n    A[Voice Command] --\x3e B(Whisper);\n    B --\x3e C{LLM Planner};\n    D[Camera Input] --\x3e C;\n    C --\x3e|Action Plan| E[ROS 2 Action Server];\n    E --\x3e F[Robot Motors];\n"})}),"\n",(0,t.jsx)(n.h2,{id:"capstone-autonomous-humanoid",children:"Capstone: Autonomous Humanoid"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Goal"}),": Build a robot that can:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Navigate a room."}),"\n",(0,t.jsx)(n.li,{children:"Find a specific object requested by voice."}),"\n",(0,t.jsx)(n.li,{children:"Approach it safely."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"(This corresponds to the final assessment of the course)"})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>r});var i=o(6540);const t={},s=i.createContext(t);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);