"use strict";(globalThis.webpackChunkcortex_h1=globalThis.webpackChunkcortex_h1||[]).push([[2357],{3152:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"module-4-vla/vision-transformers","title":"03. Vision Transformers (ViT)","description":"The Transformer architecture, originally designed for NLP, has been successfully adapted for Computer Vision, challenging the dominance of Convolutional Neural Networks (CNNs).","source":"@site/docs/module-4-vla/03-vision-transformers.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/vision-transformers","permalink":"/Cortex-H1/docs/module-4-vla/vision-transformers","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/03-vision-transformers.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"vision-transformers","title":"03. Vision Transformers (ViT)","sidebar_label":"03. Vision Transformers"},"sidebar":"textbookSidebar","previous":{"title":"4.2 RT-1 & RT-2","permalink":"/Cortex-H1/docs/module-4-vla/foundation-models"},"next":{"title":"04. Multimodal Fusion","permalink":"/Cortex-H1/docs/module-4-vla/multimodal-fusion"}}');var o=i(4848),s=i(8453);const t={id:"vision-transformers",title:"03. Vision Transformers (ViT)",sidebar_label:"03. Vision Transformers"},a="Vision Transformers (ViT)",l={},d=[{value:"How ViT Works",id:"how-vit-works",level:2},{value:"Importance for VLA",id:"importance-for-vla",level:2},{value:"CLIP (Contrastive Language-Image Pre-Training)",id:"clip-contrastive-language-image-pre-training",level:2}];function c(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"vision-transformers-vit",children:"Vision Transformers (ViT)"})}),"\n",(0,o.jsx)(n.p,{children:"The Transformer architecture, originally designed for NLP, has been successfully adapted for Computer Vision, challenging the dominance of Convolutional Neural Networks (CNNs)."}),"\n",(0,o.jsx)(n.h2,{id:"how-vit-works",children:"How ViT Works"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Patching"}),": The input image is split into fixed-size patches (e.g., 16x16 pixels)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Linear Projection"}),": Each patch is flattened and linearly projected into an embedding vector."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Positional Embeddings"}),": Information about the patch's position is added to the embedding."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Transformer Encoder"}),": The sequence of embeddings is processed by standard Transformer layers (Self-Attention and MLP)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Classification Head"}),": The output of the special [CLS] token (or global average pooling) is used for prediction."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"importance-for-vla",children:"Importance for VLA"}),"\n",(0,o.jsx)(n.p,{children:"ViTs provide a unified architecture for processing both text (tokens) and images (patches), making them ideal backbones for multimodal models."}),"\n",(0,o.jsx)(n.h2,{id:"clip-contrastive-language-image-pre-training",children:"CLIP (Contrastive Language-Image Pre-Training)"}),"\n",(0,o.jsx)(n.p,{children:"CLIP is a seminal model that learns to associate images and text by training on 400 million (image, text) pairs. It learns a joint embedding space where semantically similar images and text are close together. This zero-shot capability is crucial for open-vocabulary robot perception."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var r=i(6540);const o={},s=r.createContext(o);function t(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);