"use strict";(globalThis.webpackChunkcortex_h1=globalThis.webpackChunkcortex_h1||[]).push([[5905],{1450:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"module-3-ai-brain/mdp","title":"3.2 Lab: Custom Gym Environment","description":"Building a custom \\"GridWorld\\" MDP from scratch.","source":"@site/docs/module-3-ai-brain/02-mdp.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/mdp","permalink":"/Cortex-H1/docs/module-3-ai-brain/mdp","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-ai-brain/02-mdp.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"mdp","title":"3.2 Lab: Custom Gym Environment","sidebar_label":"3.2 Lab: Custom Env","description":"Building a custom \\"GridWorld\\" MDP from scratch."},"sidebar":"textbookSidebar","previous":{"title":"3.1 Lab: Setup","permalink":"/Cortex-H1/docs/module-3-ai-brain/intro-rl"},"next":{"title":"3.3 Lab: DQN","permalink":"/Cortex-H1/docs/module-3-ai-brain/q-learning"}}');var i=s(4848),t=s(8453);const o={id:"mdp",title:"3.2 Lab: Custom Gym Environment",sidebar_label:"3.2 Lab: Custom Env",description:'Building a custom "GridWorld" MDP from scratch.'},l="3.2 Lab: Custom Gym Environment",a={},d=[{value:"\ud83c\udfaf Lab Objectives",id:"-lab-objectives",level:2},{value:"3.2.1 The Scenario: GridWorld",id:"321-the-scenario-gridworld",level:2},{value:"3.2.2 The Code (<code>grid_world.py</code>)",id:"322-the-code-grid_worldpy",level:2},{value:"3.2.3 Quiz",id:"323-quiz",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"32-lab-custom-gym-environment",children:"3.2 Lab: Custom Gym Environment"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:'"If you can simulate it, you can solve it."'})}),"\n",(0,i.jsxs)(n.p,{children:["To apply RL to a new robot, you must wrap your robot code in a ",(0,i.jsx)(n.strong,{children:"Gym Interface"}),". This standardizes the communication between the Agent and the World."]}),"\n",(0,i.jsx)(n.h2,{id:"-lab-objectives",children:"\ud83c\udfaf Lab Objectives"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Inherit from ",(0,i.jsx)(n.code,{children:"gym.Env"})]}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Define Spaces"}),": ",(0,i.jsx)(n.code,{children:"Box"})," vs ",(0,i.jsx)(n.code,{children:"Discrete"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Implement ",(0,i.jsx)(n.code,{children:"step()"})," and ",(0,i.jsx)(n.code,{children:"reset()"})]}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"321-the-scenario-gridworld",children:"3.2.1 The Scenario: GridWorld"}),"\n",(0,i.jsx)(n.p,{children:"A simple 5x5 grid."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Start"}),": Top-Left (0,0)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Goal"}),": Bottom-Right (4,4)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Obstacle"}),": (2,2)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action"}),": Up, Down, Left, Right."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"322-the-code-grid_worldpy",children:["3.2.2 The Code (",(0,i.jsx)(n.code,{children:"grid_world.py"}),")"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass GridWorldEnv(gym.Env):\n    def __init__(self):\n        super().__init__()\n        # 5x5 Grid\n        self.grid_size = 5\n        \n        # Action Space: 0=Up, 1=Down, 2=Left, 3=Right\n        self.action_space = spaces.Discrete(4)\n        \n        # Observation Space: [x, y] coordinates\n        self.observation_space = spaces.Box(low=0, high=4, shape=(2,), dtype=np.int32)\n        \n        # State\n        self.agent_pos = np.array([0, 0])\n        self.goal_pos = np.array([4, 4])\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        self.agent_pos = np.array([0, 0])\n        return self.agent_pos, {}\n\n    def step(self, action):\n        # Physics Logic\n        if action == 0: self.agent_pos[1] = max(0, self.agent_pos[1] - 1) # Up\n        if action == 1: self.agent_pos[1] = min(4, self.agent_pos[1] + 1) # Down\n        if action == 2: self.agent_pos[0] = max(0, self.agent_pos[0] - 1) # Left\n        if action == 3: self.agent_pos[0] = min(4, self.agent_pos[0] + 1) # Right\n        \n        # Reward Function\n        terminated = np.array_equal(self.agent_pos, self.goal_pos)\n        reward = 10.0 if terminated else -0.1 # Time penalty\n        \n        return self.agent_pos, reward, terminated, False, {}\n\n# Test it\nenv = GridWorldEnv()\nobs, _ = env.reset()\nprint("Start:", obs)\nobs, reward, done, _, _ = env.step(1) # Down\nprint("Step Down:", obs, "Reward:", reward)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"323-quiz",children:"3.2.3 Quiz"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why do we give a negative reward (-0.1) for each step?"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"a) To punish the robot."}),"\n",(0,i.jsxs)(n.li,{children:["b) To encourage the robot to find the ",(0,i.jsx)(n.em,{children:"shortest"})," path (accumulate less penalty)."]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Answer: b"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["What happens if we don't define ",(0,i.jsx)(n.code,{children:"observation_space"}),"?"]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"a) Nothing."}),"\n",(0,i.jsx)(n.li,{children:"b) SB3 will crash because it needs to know the input shape for the Neural Network."}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Answer: b"})}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>l});var r=s(6540);const i={},t=r.createContext(i);function o(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);